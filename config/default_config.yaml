training:
  default_batch_size: 1
  gradient_accumulation_steps: 8
  max_seq_length: 2048
  num_epochs: 3
  early_stopping_patience: 3
  learning_rate: 5e-5
  warmup_steps: 100
  weight_decay: 0.01
  save_steps: 500
  eval_steps: 500
  logging_steps: 10
  fp16: true
  
lora:
  rank: 16
  alpha: 32
  dropout: 0.1
  target_modules:
    - q_proj
    - v_proj
    - k_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

quantization:
  default_method: "Q5_K_M"
  available_methods:
    - Q4_K_M
    - Q5_K_M
    - Q8_0
    - F16
    - F32
    
ollama:
  default_temperature: 0.7
  default_top_p: 0.9
  default_num_ctx: 4096
  default_repeat_penalty: 1.1
  
data:
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  min_text_length: 10
  max_text_length: 4096
  remove_duplicates: true
  normalize_japanese: true
  
system:
  max_memory_gb: 32
  safety_margin_gb: 2
  num_threads: 8
  use_mps: true
  
ui:
  theme: "light"
  sidebar_width: 300
  chart_height: 400
  table_page_size: 50