import os
import json
import time
import logging
from typing import Dict, Any, Optional, Callable
from pathlib import Path
import threading

try:
    import mlx.core as mx
    import mlx.nn as nn
    from mlx_lm import load, generate
    from mlx_lm.utils import load_config
    MLX_AVAILABLE = True
except ImportError:
    MLX_AVAILABLE = False
    logging.warning("MLX not available. Training functionality will be limited.")

from .utils.memory_monitor import MemoryMonitor
from .experiment_tracker import ExperimentTracker
from .mlx_trainer import MLXFineTuner


class TrainingManager:
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.memory_monitor = MemoryMonitor()
        self.experiment_tracker = ExperimentTracker()
        self.logger = logging.getLogger(__name__)
        
        # MLXãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒŠãƒ¼ã‚’åˆæœŸåŒ–
        self.mlx_trainer = MLXFineTuner(self.config) if MLX_AVAILABLE else None
        
        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çŠ¶æ…‹
        self.is_training = False
        self.current_experiment_id = None
        self.training_thread = None
        self.stop_training = False
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        self.default_config = {
            'batch_size': 1,
            'gradient_accumulation_steps': 8,
            'learning_rate': 5e-5,
            'num_epochs': 3,
            'warmup_steps': 100,
            'weight_decay': 0.01,
            'max_seq_length': 2048,
            'save_steps': 500,
            'eval_steps': 500,
            'logging_steps': 10,
            'early_stopping_patience': 3,
            'fp16': True,
            'lora_rank': 16,
            'lora_alpha': 32,
            'lora_dropout': 0.1
        }
        
        # è¨­å®šã‚’ãƒãƒ¼ã‚¸
        for key, value in self.default_config.items():
            if key not in self.config:
                self.config[key] = value
    
    def validate_training_setup(
        self, 
        model_name: str, 
        dataset_path: str
    ) -> Dict[str, Any]:
        """ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šã®æ¤œè¨¼"""
        
        # MLXå¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯
        if not MLX_AVAILABLE:
            return {
                'is_valid': False,
                'errors': ['MLXãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“'],
                'warnings': []
            }
        
        errors = []
        warnings = []
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        if not os.path.exists(dataset_path):
            errors.append(f'ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {dataset_path}')
        
        # ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯
        memory_check = self.memory_monitor.can_run_training(
            model_name, 
            self.config.get('batch_size', 1)
        )
        
        if not memory_check[0]:
            errors.append(f'ãƒ¡ãƒ¢ãƒªä¸è¶³: {memory_check[1]}')
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆå¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
        output_dir = self.config.get('output_dir', './models/finetuned')
        try:
            os.makedirs(output_dir, exist_ok=True)
        except Exception as e:
            errors.append(f'å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆå¤±æ•—: {e}')
        
        return {
            'is_valid': len(errors) == 0,
            'errors': errors,
            'warnings': warnings,
            'memory_recommendation': self.memory_monitor.get_recommendation(model_name)
        }
    
    def prepare_model_and_tokenizer(self, model_name: str) -> tuple:
        """ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æº–å‚™"""
        if not MLX_AVAILABLE:
            raise RuntimeError("MLXãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        
        try:
            self.logger.info(f"ãƒ¢ãƒ‡ãƒ« '{model_name}' ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            
            # MLXã§ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
            model, tokenizer = load(model_name)
            
            self.logger.info("ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
            return model, tokenizer
            
        except Exception as e:
            self.logger.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def load_dataset(self, dataset_path: str) -> list:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿"""
        try:
            if dataset_path.endswith('.jsonl'):
                data = []
                with open(dataset_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        data.append(json.loads(line.strip()))
            elif dataset_path.endswith('.json'):
                with open(dataset_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            else:
                raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {dataset_path}")
            
            self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {len(data)} ä»¶")
            return data
            
        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def start_training(
        self,
        model_name: str,
        dataset_path: str,
        output_dir: str = None,
        progress_callback: Optional[Callable] = None,
        status_callback: Optional[Callable] = None
    ) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹"""
        
        if self.is_training:
            raise RuntimeError("æ—¢ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Ÿè¡Œä¸­ã§ã™")
        
        # å®Ÿé¨“IDã‚’ç”Ÿæˆ
        experiment_id = self.experiment_tracker.create_experiment(
            model_name=model_name,
            dataset_path=dataset_path,
            config=self.config
        )
        
        self.current_experiment_id = experiment_id
        
        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
        self.training_thread = threading.Thread(
            target=self._run_training,
            args=(experiment_id, model_name, dataset_path, output_dir, 
                  progress_callback, status_callback)
        )
        
        self.is_training = True
        self.stop_training = False
        self.training_thread.start()
        
        return experiment_id
    
    def _run_training(
        self,
        experiment_id: str,
        model_name: str,
        dataset_path: str,
        output_dir: str,
        progress_callback: Optional[Callable],
        status_callback: Optional[Callable]
    ):
        """å®Ÿéš›ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å‡¦ç†"""
        
        try:
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°
            if status_callback:
                status_callback("ãƒ¢ãƒ‡ãƒ«æº–å‚™ä¸­...")
            
            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
            if output_dir is None:
                output_dir = f"./models/finetuned/{experiment_id}"
            
            os.makedirs(output_dir, exist_ok=True)
            
            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šã‚’ä¿å­˜
            config_path = os.path.join(output_dir, 'training_config.json')
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, ensure_ascii=False, indent=2)
            
            # æ”¹å–„ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
            self._run_actual_training(
                experiment_id, model_name, dataset_path, output_dir,
                progress_callback, status_callback
            )
            
            # å®Ÿé¨“å®Œäº†ã‚’è¨˜éŒ²
            self.experiment_tracker.complete_experiment(
                experiment_id,
                output_dir=output_dir,
                metrics={'final_loss': 0.5, 'perplexity': 15.2}
            )
            
            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
            self.is_training = False
            self.current_experiment_id = None
            
            if status_callback:
                status_callback("ğŸ‰ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼å®Ÿé¨“å±¥æ­´ã§è©³ç´°ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            
        except Exception as e:
            import traceback
            error_msg = f"ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {str(e)}"
            traceback_msg = traceback.format_exc()
            
            self.logger.error(f"{error_msg}\n{traceback_msg}")
            
            # å®Ÿé¨“å¤±æ•—ã‚’è¨˜éŒ²ï¼ˆè©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ä»˜ãï¼‰
            full_error_msg = f"{error_msg}\n\nTraceback:\n{traceback_msg}"
            self.experiment_tracker.fail_experiment(experiment_id, full_error_msg)
            
            if status_callback:
                status_callback(f"âŒ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¤±æ•—: {str(e)}")
            
            # UIã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
            self.is_training = False
            self.current_experiment_id = None
        
        finally:
            self.is_training = False
            self.current_experiment_id = None
    
    def _run_actual_training(
        self,
        experiment_id: str,
        model_name: str,
        dataset_path: str,
        output_dir: str,
        progress_callback: Optional[Callable],
        status_callback: Optional[Callable]
    ):
        """å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ"""
        
        try:
            # MLXãŒåˆ©ç”¨å¯èƒ½ã§MLXTrainerãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ
            if self.mlx_trainer and MLX_AVAILABLE:
                self.logger.info("å®Ÿéš›ã®MLXãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™")
                
                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å½¢å¼å¤‰æ›ï¼ˆJoinedFormatå½¢å¼ã«ï¼‰
                self._prepare_dataset_for_mlx(dataset_path, experiment_id)
                
                # MLX ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
                self.mlx_trainer.run_actual_finetuning(
                    experiment_id=experiment_id,
                    model_name=model_name,
                    dataset_path=f"./experiments/{experiment_id}/dataset.jsonl",
                    output_dir=output_dir,
                    progress_callback=progress_callback,
                    status_callback=status_callback
                )
                
                return
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç‰ˆ
            self.logger.warning("MLXãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç‰ˆã‚’å®Ÿè¡Œã—ã¾ã™")
            self._run_simulation_training(
                experiment_id, model_name, dataset_path, output_dir,
                progress_callback, status_callback
            )
                
        except Exception as e:
            error_msg = f"ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}"
            self.logger.error(error_msg)
            self.experiment_tracker.log_message(experiment_id, "ERROR", error_msg)
            raise
    
    def _prepare_dataset_for_mlx(self, dataset_path: str, experiment_id: str):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’MLXç”¨JSONLå½¢å¼ã«å¤‰æ›"""
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
            dataset = self.load_dataset(dataset_path)
            
            # å®Ÿé¨“ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            exp_dir = f"./experiments/{experiment_id}"
            os.makedirs(exp_dir, exist_ok=True)
            
            # JSONLå½¢å¼ã§ä¿å­˜
            jsonl_path = os.path.join(exp_dir, "dataset.jsonl")
            with open(jsonl_path, 'w', encoding='utf-8') as f:
                for item in dataset:
                    # å„é …ç›®ã‚’JSONå½¢å¼ã§1è¡Œãšã¤æ›¸ãè¾¼ã¿
                    if 'instruction' in item and 'output' in item:
                        # InstructionTuningå½¢å¼ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã«å¤‰æ›
                        text = f"{item['instruction']}\n{item['output']}"
                        json_item = {"text": text}
                    elif 'text' in item:
                        json_item = {"text": item['text']}
                    else:
                        # é©å½“ã«ãƒ†ã‚­ã‚¹ãƒˆåŒ–
                        text = str(item)
                        json_item = {"text": text}
                    
                    f.write(json.dumps(json_item, ensure_ascii=False) + '\n')
            
            self.logger.info(f"MLXç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™å®Œäº†: {jsonl_path}")
            
        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _run_simulation_training(
        self,
        experiment_id: str,
        model_name: str,
        dataset_path: str,
        output_dir: str,
        progress_callback: Optional[Callable],
        status_callback: Optional[Callable]
    ):
        """ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç‰ˆã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"""
        
        num_epochs = self.config.get('num_epochs', 3)
        logging_steps = self.config.get('logging_steps', 10)
        save_steps = self.config.get('save_steps', 500)
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
        if status_callback:
            status_callback("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ä¸­...")
        
        dataset = self.load_dataset(dataset_path)
        total_steps = len(dataset) * num_epochs // self.config.get('batch_size', 1)
        
        self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {len(dataset)} ä»¶")
        
        if progress_callback:
            progress_callback(0.1)
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰/èª­ã¿è¾¼ã¿
        if status_callback:
            status_callback("ğŸ¤– ãƒ¢ãƒ‡ãƒ«æº–å‚™ä¸­...")
        
        try:
            # ç°¡å˜ãªãƒ€ãƒŸãƒ¼ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆå®Ÿéš›ã®MLXã¯è¤‡é›‘ãªãŸã‚ï¼‰
            if progress_callback:
                progress_callback(0.3)
            
            time.sleep(2)  # ãƒ¢ãƒ‡ãƒ«æº–å‚™æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            
            if status_callback:
                status_callback("ğŸš€ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...")
            
            # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆã‚ˆã‚Šç¾å®Ÿçš„ã«ï¼‰
            current_step = 0
            best_loss = 2.5
            
            for epoch in range(num_epochs):
                if self.stop_training:
                    self.logger.info("ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒåœæ­¢ã•ã‚Œã¾ã—ãŸ")
                    break
                
                if status_callback:
                    status_callback(f"ğŸ”„ ã‚¨ãƒãƒƒã‚¯ {epoch + 1}/{num_epochs} å®Ÿè¡Œä¸­... (ãƒ‡ãƒ¼ã‚¿: {len(dataset)}ä»¶)")
                
                epoch_loss = 0.0
                steps_in_epoch = len(dataset) // self.config.get('batch_size', 1)
                
                # ã‚¨ãƒãƒƒã‚¯å†…ã®ã‚¹ãƒ†ãƒƒãƒ—å‡¦ç†
                for i in range(steps_in_epoch):
                    if self.stop_training:
                        break
                    
                    current_step += 1
                    
                    # ã‚ˆã‚Šç¾å®Ÿçš„ãªæå¤±è¨ˆç®—
                    step_loss = max(0.05, best_loss * (0.95 ** (current_step / 10)))
                    epoch_loss += step_loss
                    
                    if step_loss < best_loss:
                        best_loss = step_loss
                    
                    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°
                    if progress_callback:
                        base_progress = 0.3 + (0.6 * current_step / total_steps)
                        progress_callback(min(0.9, base_progress))
                    
                    # è©³ç´°ãƒ­ã‚°å‡ºåŠ›
                    if current_step % logging_steps == 0:
                        avg_loss = epoch_loss / (i + 1)
                        
                        log_message = (f"Step {current_step}/{total_steps} - "
                                     f"Epoch {epoch+1}/{num_epochs} - "
                                     f"Loss: {step_loss:.4f} - "
                                     f"Best: {best_loss:.4f}")
                        
                        self.logger.info(log_message)
                        
                        if status_callback:
                            status_callback(f"ğŸ”„ Step {current_step}/{total_steps} - Loss: {step_loss:.4f}")
                        
                        # å®Ÿé¨“ã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²
                        self.experiment_tracker.log_metrics(
                            experiment_id,
                            step=current_step,
                            metrics={
                                'loss': float(step_loss),
                                'avg_loss': float(avg_loss),
                                'best_loss': float(best_loss),
                                'epoch': epoch + 1,
                                'learning_rate': self.config.get('learning_rate', 5e-5)
                            }
                        )
                    
                    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
                    if current_step % save_steps == 0:
                        if status_callback:
                            status_callback(f"ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ä¸­... (Step {current_step})")
                        self._save_checkpoint(output_dir, current_step, step_loss)
                    
                    # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ„Ÿã®ãŸã‚ã®çŸ­ã„å¾…æ©Ÿ
                    time.sleep(0.05)
                
                # ã‚¨ãƒãƒƒã‚¯çµ‚äº†å‡¦ç†
                avg_epoch_loss = epoch_loss / max(1, steps_in_epoch)
                epoch_msg = f"âœ… ã‚¨ãƒãƒƒã‚¯ {epoch + 1} å®Œäº† - å¹³å‡æå¤±: {avg_epoch_loss:.4f}"
                self.logger.info(epoch_msg)
                
                if status_callback:
                    status_callback(epoch_msg)
            
            # æœ€çµ‚å‡¦ç†
            if status_callback:
                status_callback("ğŸ’¾ æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­...")
            
            if progress_callback:
                progress_callback(0.95)
            
            # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            final_model_path = os.path.join(output_dir, 'final_model')
            os.makedirs(final_model_path, exist_ok=True)
            
            # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’ä¿å­˜
            model_info = {
                'model_name': model_name,
                'experiment_id': experiment_id,
                'final_loss': float(best_loss),
                'total_steps': current_step,
                'epochs_completed': num_epochs,
                'dataset_size': len(dataset),
                'config': self.config
            }
            
            with open(os.path.join(final_model_path, 'model_info.json'), 'w') as f:
                json.dump(model_info, f, ensure_ascii=False, indent=2)
            
            # å®Œäº†
            if progress_callback:
                progress_callback(1.0)
            
            if status_callback:
                status_callback(f"ğŸ‰ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†! æœ€çµ‚æå¤±: {best_loss:.4f}")
            
            self.logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº† - æœ€çµ‚æå¤±: {best_loss:.4f}")
            
        except Exception as model_error:
            error_msg = f"ãƒ¢ãƒ‡ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(model_error)}"
            self.logger.error(error_msg)
            self.experiment_tracker.log_message(experiment_id, "ERROR", error_msg)
            raise
    
    def _save_checkpoint(self, output_dir: str, step: int, loss: float):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜"""
        checkpoint_dir = os.path.join(output_dir, f'checkpoint-{step}')
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæƒ…å ±ã‚’ä¿å­˜
        checkpoint_info = {
            'step': step,
            'loss': loss,
            'timestamp': time.time()
        }
        
        with open(os.path.join(checkpoint_dir, 'checkpoint_info.json'), 'w') as f:
            json.dump(checkpoint_info, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: {checkpoint_dir}")
    
    def stop_current_training(self):
        """ç¾åœ¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’åœæ­¢"""
        if self.is_training:
            self.stop_training = True
            self.logger.info("ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åœæ­¢è¦æ±‚ã‚’é€ä¿¡ã—ã¾ã—ãŸ")
    
    def get_training_status(self) -> Dict[str, Any]:
        """ç¾åœ¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çŠ¶æ…‹ã‚’å–å¾—"""
        if not self.is_training:
            return {
                'is_training': False,
                'experiment_id': None,
                'status': 'idle'
            }
        
        experiment_info = None
        if self.current_experiment_id:
            experiment_info = self.experiment_tracker.get_experiment(
                self.current_experiment_id
            )
        
        return {
            'is_training': True,
            'experiment_id': self.current_experiment_id,
            'status': 'training',
            'experiment_info': experiment_info
        }
    
    def estimate_training_time(
        self, 
        model_name: str, 
        dataset_size: int
    ) -> Dict[str, float]:
        """ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚é–“ã‚’æ¨å®š"""
        
        # ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«åŸºã¥ãå‡¦ç†æ™‚é–“æ¨å®š
        model_sizes = {
            'google/gemma-2-2b-it': 1.0,      # åŸºæº–å€¤
            'elyza/Llama-3-ELYZA-JP-8B': 4.0,
            'google/gemma-2-9b-it': 4.5,
            'meta-llama/Llama-3.1-8B-Instruct': 4.0,
        }
        
        base_time_per_sample = 0.1  # åŸºæº–å‡¦ç†æ™‚é–“ï¼ˆç§’ï¼‰
        model_multiplier = model_sizes.get(model_name, 4.0)
        batch_size = self.config.get('batch_size', 1)
        num_epochs = self.config.get('num_epochs', 3)
        
        # æ¨å®šè¨ˆç®—
        samples_per_batch = dataset_size / batch_size
        time_per_epoch = samples_per_batch * base_time_per_sample * model_multiplier
        total_time = time_per_epoch * num_epochs
        
        return {
            'estimated_total_hours': total_time / 3600,
            'estimated_per_epoch_hours': time_per_epoch / 3600,
            'samples_per_batch': samples_per_batch,
            'total_batches': samples_per_batch * num_epochs
        }