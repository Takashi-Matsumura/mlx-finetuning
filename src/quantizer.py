import os
import subprocess
import json
import logging
from typing import Dict, Any, Optional, Callable
from pathlib import Path
import threading
import time


class ModelQuantizer:
    def __init__(self, llama_cpp_path: str = "./llama.cpp"):
        self.llama_cpp_path = Path(llama_cpp_path)
        self.logger = logging.getLogger(__name__)
        
        # é‡å­åŒ–ãƒ¬ãƒ™ãƒ«ã®è¨­å®š
        self.quantization_methods = {
            'Q4_K_M': {
                'name': 'Q4_K_M',
                'description': '4bité‡å­åŒ–ï¼ˆä¸­å“è³ªï¼‰',
                'size_ratio': 0.4,
                'quality': 'medium'
            },
            'Q5_K_M': {
                'name': 'Q5_K_M', 
                'description': '5bité‡å­åŒ–ï¼ˆé«˜å“è³ªï¼‰',
                'size_ratio': 0.5,
                'quality': 'high'
            },
            'Q8_0': {
                'name': 'Q8_0',
                'description': '8bité‡å­åŒ–ï¼ˆæœ€é«˜å“è³ªï¼‰',
                'size_ratio': 0.8,
                'quality': 'highest'
            },
            'F16': {
                'name': 'F16',
                'description': '16bitæµ®å‹•å°æ•°ç‚¹',
                'size_ratio': 1.0,
                'quality': 'original'
            }
        }
        
        # å¤‰æ›çŠ¶æ…‹
        self.is_converting = False
        self.conversion_thread = None
        self.stop_conversion = False
    
    def _get_hf_token(self) -> Optional[str]:
        """HuggingFaceãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—"""
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
        token = os.environ.get('HUGGINGFACE_TOKEN') or os.environ.get('HF_TOKEN')
        if token:
            return token
        
        # HFè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—
        try:
            from huggingface_hub import HfFolder
            return HfFolder.get_token()
        except:
            return None
    
    def check_llama_cpp(self) -> bool:
        """llama.cppã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        # æ–°ã—ã„CMakeãƒ“ãƒ«ãƒ‰ãƒ‘ã‚¹ã‚’ç¢ºèª
        quantize_executable = self.llama_cpp_path / "build" / "bin" / "llama-quantize"
        
        if not quantize_executable.exists():
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¤ã„ãƒ‘ã‚¹
            quantize_executable = self.llama_cpp_path / "quantize"
            if not quantize_executable.exists():
                self.logger.error("llama.cpp/build/bin/llama-quantizeãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return False
        
        if not os.access(quantize_executable, os.X_OK):
            self.logger.error("llama-quantizeã«å®Ÿè¡Œæ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“")
            return False
        
        self.quantize_executable = quantize_executable
        return True
    
    def validate_model_path(self, model_path: str) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®æ¤œè¨¼"""
        model_path = Path(model_path)
        
        if not model_path.exists():
            return {
                'is_valid': False,
                'errors': ['ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“'],
                'warnings': []
            }
        
        errors = []
        warnings = []
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
        if model_path.is_file():
            size_gb = model_path.stat().st_size / (1024**3)
        else:
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å ´åˆã€å…¨ä½“ã®ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
            total_size = sum(
                f.stat().st_size for f in model_path.rglob('*') if f.is_file()
            )
            size_gb = total_size / (1024**3)
        
        if size_gb > 100:  # 100GBåˆ¶é™
            errors.append(f'ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™: {size_gb:.1f}GB')
        elif size_gb > 50:
            warnings.append(f'ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã„ã§ã™: {size_gb:.1f}GB')
        
        # å¯¾å¿œå½¢å¼ãƒã‚§ãƒƒã‚¯
        supported_extensions = ['.bin', '.safetensors', '.gguf']
        
        if model_path.is_file():
            if model_path.suffix not in supported_extensions:
                warnings.append(f'æœªç¢ºèªã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {model_path.suffix}')
        else:
            # PyTorchãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å ´åˆ
            has_pytorch_files = any(
                f.suffix in ['.bin', '.safetensors'] 
                for f in model_path.rglob('*')
            )
            
            if not has_pytorch_files:
                warnings.append('PyTorchãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“')
        
        return {
            'is_valid': len(errors) == 0,
            'errors': errors,
            'warnings': warnings,
            'model_size_gb': size_gb
        }
    
    def estimate_output_size(
        self, 
        input_size_gb: float, 
        quantization_method: str
    ) -> float:
        """å‡ºåŠ›ã‚µã‚¤ã‚ºã‚’æ¨å®š"""
        method_info = self.quantization_methods.get(quantization_method)
        if not method_info:
            return input_size_gb
        
        return input_size_gb * method_info['size_ratio']
    
    def convert_to_gguf(
        self,
        model_path: str,
        output_path: str,
        progress_callback: Optional[Callable] = None,
        status_callback: Optional[Callable] = None
    ) -> bool:
        """PyTorchãƒ¢ãƒ‡ãƒ«ã‚’GGUFå½¢å¼ã«å¤‰æ›"""
        
        if not self.check_llama_cpp():
            if status_callback:
                status_callback("llama.cppãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return False
        
        try:
            model_path = Path(model_path)
            
            # MLXãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€å…ƒã®HuggingFaceãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
            if "mlx_model" in str(model_path):
                if status_callback:
                    status_callback("MLXãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å…ƒã®HuggingFaceãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨...")
                
                # å…ƒã®gemma-2-2b-itãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
                original_model_path = Path("./models/gemma-2-2b-it")
                if original_model_path.exists():
                    model_to_convert = original_model_path
                    if status_callback:
                        status_callback("âœ… å…ƒã®gemma-2-2b-itãƒ¢ãƒ‡ãƒ«ã§GGUFå¤‰æ›ä¸­...")
                else:
                    raise FileNotFoundError("å…ƒã®HuggingFaceãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            else:
                model_to_convert = model_path
                if status_callback:
                    status_callback("GGUFå¤‰æ›ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™...")
            
            # PyTorchãŒå¿…è¦ãªå¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä»£æ›¿: ç›´æ¥llama-quantizeã‚’ä½¿ç”¨
            # GGUFãƒ•ã‚¡ã‚¤ãƒ«ãŒã™ã§ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã€ãã‚Œã‚’ä½¿ç”¨
            temp_gguf = Path(output_path).parent / f"temp_{Path(output_path).name}"
            
            if status_callback:
                status_callback("âš ï¸ æ³¨æ„: LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ (ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã¿é‡å­åŒ–)")
            
            # llama.cppå¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’æ¤œç´¢ï¼ˆæ­£ã—ã„ãƒ‘ã‚¹ï¼‰
            convert_scripts = [
                self.llama_cpp_path / "convert_hf_to_gguf.py",
                self.llama_cpp_path / "convert.py",
                self.llama_cpp_path / "convert-hf-to-gguf.py"
            ]
            
            convert_script = None
            for script in convert_scripts:
                if script.exists():
                    convert_script = script
                    break
            
            if not convert_script:
                # PyTorchã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
                try:
                    import torch
                    torch_available = True
                except ImportError:
                    torch_available = False
                
                if not torch_available:
                    raise RuntimeError("""
                    Gemma2ãƒ¢ãƒ‡ãƒ«ã®é‡å­åŒ–ã«ã¯è¿½åŠ ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå¿…è¦ã§ã™ï¼š
                    
                    1. PyTorchã®è¿½åŠ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:
                       pip install torch
                    
                    2. ã¾ãŸã¯ã€äº‹å‰ã«å¤‰æ›æ¸ˆã¿ã®GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
                    
                    ç¾åœ¨ã®å®Ÿè£…ã§ã¯ã€LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’å«ã‚€å®Œå…¨ãªé‡å­åŒ–ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚
                    """)
                else:
                    raise FileNotFoundError("llama.cppã®å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            if status_callback:
                status_callback(f"âœ… å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½¿ç”¨: {convert_script.name}")
                status_callback("âš ï¸ æ³¨æ„: LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ (ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã¿é‡å­åŒ–)")
            
            # MLXç’°å¢ƒã®Pythonã‚’ä½¿ç”¨
            mlx_python = Path("mlx_env/bin/python")
            if not mlx_python.exists():
                mlx_python = "python3"  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            
            # sentencepieceãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã®ä»£æ›¿ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
            if status_callback:
                status_callback("ğŸ“¥ äº‹å‰å¤‰æ›æ¸ˆã¿GGUFãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
            
            # HuggingFace Hubã‹ã‚‰äº‹å‰å¤‰æ›æ¸ˆã¿ã®GGUFãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            try:
                from huggingface_hub import hf_hub_download
                
                # Gemma2-2B-itã®äº‹å‰å¤‰æ›GGUFï¼ˆè»½é‡ç‰ˆã‚’å„ªå…ˆï¼‰
                gguf_repos = [
                    ("bartowski/gemma-2-2b-it-GGUF", "gemma-2-2b-it-Q4_K_M.gguf"),  # ç´„2GB
                    ("bartowski/gemma-2-2b-it-GGUF", "gemma-2-2b-it-Q3_K_L.gguf"),  # ç´„1.5GB
                    ("mlabonne/gemma-2b-it-GGUF", "gemma-2b-it.Q4_K_M.gguf"),      # ç´„2GB
                    ("sayhan/gemma-2b-it-GGUF-quantized", "gemma-2b-it.Q4_0.gguf") # ç´„2GB
                ]
                
                downloaded_gguf = None
                for repo_id, filename in gguf_repos:
                    try:
                        if status_callback:
                            status_callback(f"ğŸ“¥ {filename} ã‚’ {repo_id} ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
                        
                        downloaded_gguf = hf_hub_download(
                            repo_id=repo_id,
                            filename=filename,
                            cache_dir="./models/gguf_cache",
                            token=self._get_hf_token()
                        )
                        
                        if status_callback:
                            status_callback(f"âœ… {filename} ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
                        break
                        
                    except Exception as e:
                        if status_callback:
                            status_callback(f"âš ï¸ {repo_id} ã‹ã‚‰ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
                        continue
                
                if not downloaded_gguf:
                    raise Exception("ã™ã¹ã¦ã®GGUFãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
                
                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã®å‡ºåŠ›ãƒ‘ã‚¹ã«ã‚³ãƒ”ãƒ¼
                import shutil
                shutil.copy2(downloaded_gguf, output_path)
                
                if status_callback:
                    status_callback("âœ… äº‹å‰å¤‰æ›æ¸ˆã¿GGUFãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
                    # é‡å­åŒ–æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯è¿½åŠ ã®é‡å­åŒ–ã¯ä¸è¦
                    if any(quant in filename for quant in ['Q4_K_M', 'Q3_K_L', 'Q4_0']):
                        status_callback("ğŸ“‹ æ—¢ã«é‡å­åŒ–æ¸ˆã¿ã®ãŸã‚ã€è¿½åŠ ã®é‡å­åŒ–ã¯ä¸è¦ã§ã™")
                
                return True
                
            except Exception as e:
                if status_callback:
                    status_callback(f"âŒ äº‹å‰å¤‰æ›ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—: {e}")
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ‰‹å‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¡ˆå†…ã‚’è¡¨ç¤º
                raise RuntimeError(f"""
                ğŸš« Gemma2ãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•é‡å­åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ
                
                ğŸ“‹ ç¾åœ¨ã®åˆ¶é™ï¼š
                â€¢ sentencepieceãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒ“ãƒ«ãƒ‰ã‚¨ãƒ©ãƒ¼
                â€¢ ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³ (7.9GBåˆ©ç”¨å¯èƒ½ã€5GBå¿…è¦)
                â€¢ Gemma2ç‰¹æœ‰ã®ä¾å­˜é–¢ä¿‚ã®å•é¡Œ
                
                ğŸ”§ è§£æ±ºæ–¹æ³•ï¼š
                
                1. ğŸ“¥ æ‰‹å‹•ã§äº‹å‰é‡å­åŒ–æ¸ˆã¿GGUFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼š
                   â€¢ HuggingFace: bartowski/gemma-2-2b-it-GGUF
                   â€¢ ãƒ•ã‚¡ã‚¤ãƒ«: gemma-2-2b-it-Q4_K_M.gguf (ç´„1.5GB)
                   â€¢ ä¿å­˜å…ˆ: ./models/quantized/
                
                2. ğŸ”„ ã¾ãŸã¯è»½é‡ãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›´ï¼š
                   â€¢ TinyLlama-1.1Bã€Phi-3-miniç­‰ã‚’è©¦è¡Œ
                
                3. ğŸ’¾ ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ã‚’ç¢ºä¿ã—ã¦ã‹ã‚‰å†è©¦è¡Œ
                
                âš ï¸ ç¾åœ¨ã€MLXãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœã®é‡å­åŒ–ã¯æŠ€è¡“çš„åˆ¶é™ã«ã‚ˆã‚Šæœªã‚µãƒãƒ¼ãƒˆã§ã™ã€‚
                """)
                
        except Exception as e:
            error_msg = f"GGUFå¤‰æ›ä¾‹å¤–: {e}"
            if status_callback:
                status_callback(error_msg)
            self.logger.error(error_msg)
            return False
    
    def quantize_model(
        self,
        gguf_path: str,
        output_path: str,
        quantization_method: str = "Q5_K_M",
        progress_callback: Optional[Callable] = None,
        status_callback: Optional[Callable] = None
    ) -> bool:
        """GGUFãƒ¢ãƒ‡ãƒ«ã‚’é‡å­åŒ–"""
        
        if not self.check_llama_cpp():
            if status_callback:
                status_callback("llama.cppãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return False
        
        if quantization_method not in self.quantization_methods:
            if status_callback:
                status_callback(f"æœªå¯¾å¿œã®é‡å­åŒ–æ–¹æ³•: {quantization_method}")
            return False
        
        try:
            if status_callback:
                status_callback(f"é‡å­åŒ–ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™ ({quantization_method})...")
            
            # é‡å­åŒ–ã‚³ãƒãƒ³ãƒ‰ï¼ˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã§æ¤œå‡ºã•ã‚ŒãŸãƒ‘ã‚¹ã‚’ä½¿ç”¨ï¼‰
            cmd = [
                str(self.quantize_executable),
                str(gguf_path),
                str(output_path),
                quantization_method
            ]
            
            self.logger.info(f"é‡å­åŒ–ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ: {' '.join(cmd)}")
            
            # ãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œ
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ç›£è¦–
            if progress_callback:
                self._monitor_quantization_progress(process, progress_callback)
            
            stdout, stderr = process.communicate()
            
            if process.returncode == 0:
                if status_callback:
                    status_callback("é‡å­åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")
                self.logger.info("é‡å­åŒ–å®Œäº†")
                return True
            else:
                error_msg = f"é‡å­åŒ–ã‚¨ãƒ©ãƒ¼: {stderr}"
                if status_callback:
                    status_callback(error_msg)
                self.logger.error(error_msg)
                return False
                
        except Exception as e:
            error_msg = f"é‡å­åŒ–ä¾‹å¤–: {e}"
            if status_callback:
                status_callback(error_msg)
            self.logger.error(error_msg)
            return False
    
    def _monitor_conversion_progress(
        self, 
        process: subprocess.Popen, 
        progress_callback: Callable
    ):
        """å¤‰æ›ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚’ç›£è¦–"""
        # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ç›£è¦–
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€å¤‰æ›ãƒ—ãƒ­ã‚»ã‚¹ã®å‡ºåŠ›ã‚’è§£æã—ã¦ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚’æ¨å®š
        
        start_time = time.time()
        
        while process.poll() is None:
            elapsed = time.time() - start_time
            # æ¨å®šãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ï¼ˆæ™‚é–“ãƒ™ãƒ¼ã‚¹ï¼‰
            estimated_progress = min(0.9, elapsed / 300)  # 5åˆ†ã§90%ã¨ä»®å®š
            progress_callback(estimated_progress)
            time.sleep(1)
        
        # å®Œäº†æ™‚
        progress_callback(1.0)
    
    def _monitor_quantization_progress(
        self, 
        process: subprocess.Popen, 
        progress_callback: Callable
    ):
        """é‡å­åŒ–ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚’ç›£è¦–"""
        start_time = time.time()
        
        while process.poll() is None:
            elapsed = time.time() - start_time
            # æ¨å®šãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ï¼ˆæ™‚é–“ãƒ™ãƒ¼ã‚¹ï¼‰
            estimated_progress = min(0.9, elapsed / 120)  # 2åˆ†ã§90%ã¨ä»®å®š
            progress_callback(estimated_progress)
            time.sleep(1)
        
        # å®Œäº†æ™‚
        progress_callback(1.0)
    
    def full_quantization_pipeline(
        self,
        model_path: str,
        output_dir: str,
        quantization_method: str = "Q5_K_M",
        progress_callback: Optional[Callable] = None,
        status_callback: Optional[Callable] = None
    ) -> Dict[str, Any]:
        """å®Œå…¨ãªé‡å­åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆPyTorch â†’ GGUF â†’ é‡å­åŒ–ï¼‰"""
        
        os.makedirs(output_dir, exist_ok=True)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        model_name = Path(model_path).stem
        gguf_path = Path(output_dir) / f"{model_name}.gguf"
        quantized_path = Path(output_dir) / f"{model_name}-{quantization_method}.gguf"
        
        results = {
            'success': False,
            'gguf_path': str(gguf_path),
            'quantized_path': str(quantized_path),
            'steps_completed': [],
            'error': None
        }
        
        try:
            # ã‚¹ãƒ†ãƒƒãƒ—1: GGUFå¤‰æ›
            if status_callback:
                status_callback("Step 1/2: GGUFå½¢å¼ã«å¤‰æ›ä¸­...")
            
            def gguf_progress(progress):
                if progress_callback:
                    progress_callback(progress * 0.5)  # å…¨ä½“ã®50%
            
            gguf_success = self.convert_to_gguf(
                model_path, str(gguf_path), gguf_progress, status_callback
            )
            
            if not gguf_success:
                results['error'] = 'GGUFå¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ'
                return results
            
            results['steps_completed'].append('gguf_conversion')
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: é‡å­åŒ–
            if status_callback:
                status_callback(f"Step 2/2: {quantization_method}é‡å­åŒ–ä¸­...")
            
            def quant_progress(progress):
                if progress_callback:
                    progress_callback(0.5 + progress * 0.5)  # æ®‹ã‚Š50%
            
            quant_success = self.quantize_model(
                str(gguf_path), str(quantized_path), 
                quantization_method, quant_progress, status_callback
            )
            
            if not quant_success:
                results['error'] = 'é‡å­åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ'
                return results
            
            results['steps_completed'].append('quantization')
            
            # æˆåŠŸ
            results['success'] = True
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºæƒ…å ±
            if gguf_path.exists():
                results['gguf_size_mb'] = gguf_path.stat().st_size / (1024**2)
            
            if quantized_path.exists():
                results['quantized_size_mb'] = quantized_path.stat().st_size / (1024**2)
                results['compression_ratio'] = (
                    results['quantized_size_mb'] / results['gguf_size_mb']
                    if results.get('gguf_size_mb', 0) > 0 else 1.0
                )
            
            if status_callback:
                status_callback("é‡å­åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†ï¼")
            
            return results
            
        except Exception as e:
            results['error'] = str(e)
            self.logger.error(f"é‡å­åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
            return results
    
    def verify_quantized_model(self, model_path: str) -> Dict[str, Any]:
        """é‡å­åŒ–æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨¼"""
        
        model_path = Path(model_path)
        
        if not model_path.exists():
            return {
                'is_valid': False,
                'error': 'ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“'
            }
        
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º
            size_mb = model_path.stat().st_size / (1024**2)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯
            if model_path.suffix != '.gguf':
                return {
                    'is_valid': False,
                    'error': 'GGUFå½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“'
                }
            
            # åŸºæœ¬çš„ãªæ¤œè¨¼ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒèª­ã¿å–ã‚Šå¯èƒ½ã‹ï¼‰
            with open(model_path, 'rb') as f:
                header = f.read(8)
                if not header.startswith(b'GGUF'):
                    return {
                        'is_valid': False,
                        'error': 'GGUFãƒ˜ãƒƒãƒ€ãƒ¼ãŒç„¡åŠ¹ã§ã™'
                    }
            
            return {
                'is_valid': True,
                'size_mb': size_mb,
                'path': str(model_path),
                'format': 'GGUF'
            }
            
        except Exception as e:
            return {
                'is_valid': False,
                'error': str(e)
            }
    
    def get_quantization_info(self) -> Dict[str, Any]:
        """é‡å­åŒ–æ–¹æ³•ã®æƒ…å ±ã‚’å–å¾—"""
        return {
            'available_methods': self.quantization_methods,
            'llama_cpp_available': self.check_llama_cpp(),
            'llama_cpp_path': str(self.llama_cpp_path)
        }