import streamlit as st
import yaml
import json
import os
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from pathlib import Path
import logging
import tarfile
import shutil
from datetime import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.data_processor import DatasetProcessor
from src.trainer import TrainingManager
from src.quantizer import ModelQuantizer
from src.ollama_integration import OllamaIntegrator
from src.experiment_tracker import ExperimentTracker
from src.utils.memory_monitor import MemoryMonitor
from src.utils.validators import validate_all
from src.smart_recommender import SmartParameterRecommender
from typing import Dict, Any, List


def create_transfer_archive(experiment_id: str, output_name: str) -> Dict[str, Any]:
    """è»¢é€ç”¨ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚’ä½œæˆ"""
    try:
        # ãƒ‘ã‚¹è¨­å®š
        finetuned_dir = Path(f"./models/finetuned/{experiment_id}")
        quantized_dir = Path("./models/quantized")
        experiments_dir = Path(f"./experiments/{experiment_id}")
        output_path = Path(f"./{output_name}.tar.gz")
        
        # å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
        required_files = []
        
        # 1. LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼
        adapters_file = finetuned_dir / "adapters.safetensors"
        if adapters_file.exists():
            required_files.append(("models/finetuned", adapters_file))
        
        # 2. ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼è¨­å®š
        adapter_config = finetuned_dir / "adapter_config.json"
        if adapter_config.exists():
            required_files.append(("models/finetuned", adapter_config))
        
        # 3. é‡å­åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        gguf_file = None
        
        # ã¾ãšã€MLXãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰IDã‚’å–å¾—
        mlx_model_dir = None
        for mlx_dir in finetuned_dir.glob("mlx_model_*"):
            mlx_model_dir = mlx_dir
            break
        
        if mlx_model_dir and mlx_model_dir.exists():
            # MLXãƒ¢ãƒ‡ãƒ«IDã‚’æŠ½å‡º
            mlx_id = mlx_model_dir.name.replace("mlx_model_", "")
            
            # Q5_K_Mé‡å­åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å„ªå…ˆçš„ã«æ¢ã™
            for priority_suffix in ["-Q5_K_M.gguf", "-Q4_K_M.gguf", ".gguf"]:
                candidate_file = quantized_dir / f"mlx_model_{mlx_id}{priority_suffix}"
                if candidate_file.exists():
                    gguf_file = candidate_file
                    break
            
            # ãã‚Œã§ã‚‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€globæ¤œç´¢
            if not gguf_file:
                for gguf_path in quantized_dir.glob(f"*{mlx_id}*.gguf"):
                    gguf_file = gguf_path
                    break
        
        if gguf_file and gguf_file.exists():
            required_files.append(("models/quantized", gguf_file))
        
        # 4. å®Ÿé¨“è¨­å®š
        exp_info = experiments_dir / "experiment_info.json"
        if exp_info.exists():
            required_files.append(("experiments", exp_info))
        
        if not required_files:
            return {
                'success': False,
                'error': f'å®Ÿé¨“ {experiment_id} ã®å¿…è¦ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'
            }
        
        # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä½œæˆ
        with tarfile.open(output_path, 'w:gz') as tar:
            for category, file_path in required_files:
                # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å†…ã§ã®ç›¸å¯¾ãƒ‘ã‚¹
                if category == "models/finetuned":
                    arcname = f"models/finetuned/{experiment_id}/{file_path.name}"
                elif category == "models/quantized":
                    arcname = f"models/quantized/{file_path.name}"
                elif category == "experiments":
                    arcname = f"experiments/{experiment_id}/{file_path.name}"
                
                tar.add(file_path, arcname=arcname)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º
        size_mb = output_path.stat().st_size / (1024**2)
        
        return {
            'success': True,
            'archive_path': str(output_path.absolute()),
            'filename': output_path.name,
            'size_mb': size_mb,
            'gguf_filename': gguf_file.name if gguf_file else 'model.gguf',
            'files_included': len(required_files)
        }
        
    except Exception as e:
        return {
            'success': False,
            'error': str(e)
        }


def get_cleanup_info() -> Dict[str, float]:
    """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¯¾è±¡ã®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æƒ…å ±ã‚’å–å¾—"""
    
    def get_dir_size(path: Path) -> float:
        """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚µã‚¤ã‚ºã‚’GBå˜ä½ã§å–å¾—"""
        if not path.exists():
            return 0.0
        
        total_size = 0
        try:
            for file_path in path.rglob('*'):
                if file_path.is_file():
                    total_size += file_path.stat().st_size
        except (OSError, PermissionError):
            pass
        
        return total_size / (1024**3)
    
    return {
        'finetuned_size_gb': get_dir_size(Path("./models/finetuned")),
        'quantized_size_gb': get_dir_size(Path("./models/quantized")),
        'experiments_size_gb': get_dir_size(Path("./experiments")),
        'mlx_cache_size_gb': get_dir_size(Path("./models/cache")),
        'gguf_cache_size_gb': get_dir_size(Path("./models/gguf_cache"))
    }


def perform_cleanup(cleanup_options: List[str]) -> Dict[str, Any]:
    """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ"""
    try:
        total_freed = 0.0
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‰ã®ã‚µã‚¤ã‚ºè¨˜éŒ²
        cleanup_info_before = get_cleanup_info()
        
        for option in cleanup_options:
            if "ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœ" in option:
                target_dir = Path("./models/finetuned")
                if target_dir.exists():
                    total_freed += cleanup_info_before['finetuned_size_gb']
                    shutil.rmtree(target_dir)
                    target_dir.mkdir(exist_ok=True)
            
            elif "é‡å­åŒ–ãƒ•ã‚¡ã‚¤ãƒ«" in option:
                target_dir = Path("./models/quantized")
                if target_dir.exists():
                    total_freed += cleanup_info_before['quantized_size_gb']
                    shutil.rmtree(target_dir)
                    target_dir.mkdir(exist_ok=True)
            
            elif "å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿" in option:
                target_dir = Path("./experiments")
                if target_dir.exists():
                    total_freed += cleanup_info_before['experiments_size_gb']
                    # metadata ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä¿æŒ
                    for item in target_dir.iterdir():
                        if item.is_dir():
                            shutil.rmtree(item)
                        elif item.name != "experiments_metadata.json":
                            item.unlink()
            
            elif "MLXã‚­ãƒ£ãƒƒã‚·ãƒ¥" in option:
                target_dir = Path("./models/cache")
                if target_dir.exists():
                    total_freed += cleanup_info_before['mlx_cache_size_gb']
                    shutil.rmtree(target_dir)
                    target_dir.mkdir(exist_ok=True)
            
            elif "GGUFã‚­ãƒ£ãƒƒã‚·ãƒ¥" in option:
                target_dir = Path("./models/gguf_cache")
                if target_dir.exists():
                    total_freed += cleanup_info_before['gguf_cache_size_gb']
                    shutil.rmtree(target_dir)
                    target_dir.mkdir(exist_ok=True)
        
        return {
            'success': True,
            'freed_gb': total_freed
        }
        
    except Exception as e:
        return {
            'success': False,
            'error': str(e)
        }


# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="LLM ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° ã‚¢ãƒ—ãƒª",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'training_manager' not in st.session_state:
    st.session_state['training_manager'] = None
if 'current_experiment_id' not in st.session_state:
    st.session_state['current_experiment_id'] = None


def get_disk_usage():
    """ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡ã‚’å–å¾—"""
    import shutil
    
    try:
        total, used, free = shutil.disk_usage(".")
        
        total_gb = total / (1024**3)
        used_gb = used / (1024**3) 
        free_gb = free / (1024**3)
        usage_percent = (used / total) * 100
        
        return {
            'total_gb': total_gb,
            'used_gb': used_gb,
            'free_gb': free_gb,
            'usage_percent': usage_percent
        }
    except Exception as e:
        logger.error(f"ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return {
            'total_gb': 0,
            'used_gb': 0,
            'free_gb': 0,
            'usage_percent': 0
        }


def cleanup_docker():
    """Dockerã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ"""
    import subprocess
    
    try:
        with st.spinner("ğŸ³ Dockerã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œä¸­..."):
            results = []
            
            # æœªä½¿ç”¨ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’å‰Šé™¤
            result = subprocess.run(
                ["docker", "image", "prune", "-a", "-f"],
                capture_output=True, text=True
            )
            if result.returncode == 0:
                results.append(f"Images: {result.stdout.split('Total reclaimed space: ')[-1].strip()}")
            
            # æœªä½¿ç”¨ã‚³ãƒ³ãƒ†ãƒŠã‚’å‰Šé™¤
            result = subprocess.run(
                ["docker", "container", "prune", "-f"],
                capture_output=True, text=True
            )
            if result.returncode == 0 and "Total reclaimed space: " in result.stdout:
                results.append(f"Containers: {result.stdout.split('Total reclaimed space: ')[-1].strip()}")
            
            # æœªä½¿ç”¨ãƒœãƒªãƒ¥ãƒ¼ãƒ ã‚’å‰Šé™¤
            result = subprocess.run(
                ["docker", "volume", "prune", "-f"],
                capture_output=True, text=True
            )
            if result.returncode == 0 and "Total reclaimed space: " in result.stdout:
                results.append(f"Volumes: {result.stdout.split('Total reclaimed space: ')[-1].strip()}")
            
            # ãƒ“ãƒ«ãƒ‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤
            result = subprocess.run(
                ["docker", "builder", "prune", "-a", "-f"],
                capture_output=True, text=True
            )
            if result.returncode == 0 and "Total:" in result.stdout:
                cache_size = result.stdout.split("Total:")[-1].strip()
                results.append(f"Build Cache: {cache_size}")
            
            if results:
                st.success(f"âœ… Dockerã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†!\n\n" + "\n".join(results))
            else:
                st.info("â„¹ï¸ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¯¾è±¡ã®Dockerãƒªã‚½ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                
    except FileNotFoundError:
        st.warning("âš ï¸ DockerãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚DockerãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    except Exception as e:
        st.error(f"âŒ Dockerã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")


def cleanup_temp_files():
    """ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ"""
    import subprocess
    
    try:
        with st.spinner("ğŸ—‚ï¸ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ä¸­..."):
            deleted_files = 0
            
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            temp_patterns = [
                "*.log", "*.tmp", "__pycache__", ".DS_Store", 
                "*.pyc", ".pytest_cache", ".coverage"
            ]
            
            for pattern in temp_patterns:
                result = subprocess.run(
                    ["find", ".", "-name", pattern, "-type", "f", "-delete"],
                    capture_output=True, text=True
                )
                if result.returncode == 0:
                    # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆæ¦‚ç®—ï¼‰
                    count_result = subprocess.run(
                        ["find", ".", "-name", pattern, "-type", "f"],
                        capture_output=True, text=True
                    )
                    deleted_files += len([l for l in count_result.stdout.split('\n') if l.strip()])
            
            # ã‚·ã‚¹ãƒ†ãƒ ã®ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆå®‰å…¨ãªç¯„å›²ã§ï¼‰
            subprocess.run(["rm", "-rf", "/tmp/streamlit-*"], capture_output=True)
            subprocess.run(["rm", "-rf", "/tmp/mlx_*"], capture_output=True)
            
            st.success(f"âœ… ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤å®Œäº†! ç´„{deleted_files}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¾ã—ãŸ")
            
    except Exception as e:
        st.error(f"âŒ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")


def cleanup_quantization_files():
    """é‡å­åŒ–é–¢é€£ã®ä¸è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    try:
        with st.spinner("ğŸ“¦ é‡å­åŒ–ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†ä¸­..."):
            cleaned_size = 0
            
            # models/quantizedãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç ´æãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªãƒ»å‰Šé™¤
            quantized_dir = Path("./models/quantized")
            if quantized_dir.exists():
                for file in quantized_dir.glob("*.gguf"):
                    try:
                        # GGUFãƒ•ã‚¡ã‚¤ãƒ«ã®ç°¡æ˜“ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€åˆã®4ãƒã‚¤ãƒˆãŒGGUFï¼‰
                        with open(file, 'rb') as f:
                            header = f.read(4)
                            if header != b'GGUF':
                                file_size = file.stat().st_size / (1024**3)  # GB
                                file.unlink()
                                cleaned_size += file_size
                                st.warning(f"ç ´æãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤: {file.name}")
                    except Exception:
                        # èª­ã¿å–ã‚Šã§ããªã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å‰Šé™¤
                        try:
                            file_size = file.stat().st_size / (1024**3)
                            file.unlink()
                            cleaned_size += file_size
                            st.warning(f"ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤: {file.name}")
                        except:
                            pass
            
            # MLXã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            mlx_cache_dirs = [
                Path("./models/cache"),
                Path("./models/.cache"),
                Path("~/.cache/mlx").expanduser(),
                Path("~/.cache/huggingface").expanduser()
            ]
            
            for cache_dir in mlx_cache_dirs:
                if cache_dir.exists():
                    try:
                        import shutil
                        dir_size = sum(f.stat().st_size for f in cache_dir.rglob('*') if f.is_file()) / (1024**3)
                        shutil.rmtree(cache_dir, ignore_errors=True)
                        cleaned_size += dir_size
                    except:
                        pass
            
            if cleaned_size > 0:
                st.success(f"âœ… é‡å­åŒ–ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†å®Œäº†! {cleaned_size:.2f}GBå‰Šé™¤ã—ã¾ã—ãŸ")
            else:
                st.info("â„¹ï¸ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¯¾è±¡ã®é‡å­åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                
    except Exception as e:
        st.error(f"âŒ é‡å­åŒ–ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†ã‚¨ãƒ©ãƒ¼: {e}")


def load_config():
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    try:
        with open('./config/default_config.yaml', 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        with open('./config/models.yaml', 'r', encoding='utf-8') as f:
            models_config = yaml.safe_load(f)
        
        return config, models_config
    except Exception as e:
        st.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {}, {}


def sidebar_menu():
    """ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒ¡ãƒ‹ãƒ¥ãƒ¼"""
    with st.sidebar:
        st.title("ğŸš€ LLM ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°")
        st.markdown("---")
        
        menu_options = [
            ("ğŸ  ãƒ›ãƒ¼ãƒ ", "home"),
            ("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™", "dataset"),
            ("ğŸš€ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°", "training"),
            ("ğŸ“¦ é‡å­åŒ–", "quantization"),
            ("ğŸ¤– Ollamaçµ±åˆ", "ollama"),
            ("ğŸ“ˆ å®Ÿé¨“å±¥æ­´", "experiments"),
            ("âš™ï¸ è¨­å®š", "settings")
        ]
        
        selected_page = st.radio(
            "ãƒ¡ãƒ‹ãƒ¥ãƒ¼",
            options=[option[1] for option in menu_options],
            format_func=lambda x: next(option[0] for option in menu_options if option[1] == x),
            key="sidebar_menu"
        )
        
        st.markdown("---")
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        memory_monitor = MemoryMonitor()
        memory_info = memory_monitor.get_memory_info()
        
        st.subheader("ğŸ’» ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±")
        st.metric("ä½¿ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª", f"{memory_info['available_gb']:.1f} GB")
        st.metric("ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡", f"{memory_info['percent']:.1f}%")
        
        return selected_page


def home_page():
    """ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸"""
    st.title("ğŸš€ LLM ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° ã‚¢ãƒ—ãƒª")
    st.markdown("MacBook Air M4ç”¨ã®æ—¥æœ¬èªLLMãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°è‡ªå‹•åŒ–ãƒ„ãƒ¼ãƒ«")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡¦ç†", "è‡ªå‹•åŒ–", help="CSV/JSONãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•å‰å‡¦ç†")
        
    with col2:
        st.metric("ğŸš€ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°", "MLXå¯¾å¿œ", help="Apple Siliconæœ€é©åŒ–")
        
    with col3:
        st.metric("ğŸ“¦ é‡å­åŒ–", "GGUFå¤‰æ›", help="Ollamaçµ±åˆå¯¾å¿œ")
    
    st.markdown("---")
    
    # ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰
    st.subheader("ğŸ“‹ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ")
    
    with st.expander("1ï¸âƒ£ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™", expanded=True):
        st.markdown("""
        - CSV/JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        - è‡ªå‹•çš„ãªæ—¥æœ¬èªæ­£è¦åŒ–
        - å“è³ªæ¤œè¨¼ã¨ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        """)
    
    with st.expander("2ï¸âƒ£ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ"):
        st.markdown("""
        - ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆELYZA/Gemma2ï¼‰
        - LoRAãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š
        - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—ç›£è¦–
        """)
    
    with st.expander("3ï¸âƒ£ é‡å­åŒ–ã¨çµ±åˆ"):
        st.markdown("""
        - GGUFå½¢å¼ã¸ã®å¤‰æ›
        - é‡å­åŒ–å‡¦ç†ï¼ˆQ4/Q5/Q8ï¼‰
        - Ollamaã¸ã®è‡ªå‹•ç™»éŒ²
        """)
    
    # æœ€è¿‘ã®å®Ÿé¨“
    st.subheader("ğŸ“ˆ æœ€è¿‘ã®å®Ÿé¨“")
    experiment_tracker = ExperimentTracker()
    recent_experiments = experiment_tracker.list_experiments(limit=5)
    
    if recent_experiments:
        df = pd.DataFrame([
            {
                'ID': exp['id'][:8],
                'ãƒ¢ãƒ‡ãƒ«': exp['model_name'].split('/')[-1],
                'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹': exp['status'],
                'ä½œæˆæ—¥æ™‚': exp['created_at'][:19]
            }
            for exp in recent_experiments
        ])
        st.dataframe(df, use_container_width=True)
    else:
        st.info("ã¾ã å®Ÿé¨“ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“")


def dataset_page():
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ãƒšãƒ¼ã‚¸"""
    st.title("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™")
    
    # æ—¢å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç®¡ç†ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    st.subheader("ğŸ—‚ï¸ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç®¡ç†")
    
    processed_data_dir = Path("./data/processed")
    
    if processed_data_dir.exists():
        # æ—¢å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä¸€è¦§å–å¾—
        dataset_dirs = [d for d in processed_data_dir.iterdir() if d.is_dir()]
        
        if dataset_dirs:
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæƒ…å ±ã‚’è¡¨ç¤º
            dir_info = []
            for dir_path in dataset_dirs:
                # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã¨ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
                file_count = sum(1 for f in dir_path.rglob('*') if f.is_file())
                total_size = sum(f.stat().st_size for f in dir_path.rglob('*') if f.is_file())
                total_size_mb = total_size / (1024 * 1024)
                
                # ä½œæˆæ—¥æ™‚ã‚’å–å¾—
                creation_time = datetime.fromtimestamp(dir_path.stat().st_ctime)
                
                dir_info.append({
                    'ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå': dir_path.name,
                    'ãƒ•ã‚¡ã‚¤ãƒ«æ•°': file_count,
                    'ã‚µã‚¤ã‚º(MB)': f"{total_size_mb:.1f}",
                    'ä½œæˆæ—¥æ™‚': creation_time.strftime('%Y-%m-%d %H:%M:%S'),
                    'ãƒ‘ã‚¹': str(dir_path)
                })
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã§è¡¨ç¤º
            df = pd.DataFrame(dir_info)
            st.dataframe(df.drop('ãƒ‘ã‚¹', axis=1), use_container_width=True)
            
            # å‰Šé™¤æ©Ÿèƒ½
            with st.expander("ğŸ—‘ï¸ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‰Šé™¤", expanded=False):
                selected_dirs = st.multiselect(
                    "å‰Šé™¤ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’é¸æŠ",
                    options=[info['ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå'] for info in dir_info],
                    help="è¤‡æ•°é¸æŠå¯èƒ½ã§ã™ã€‚å‰Šé™¤ã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯å¾©å…ƒã§ãã¾ã›ã‚“ã€‚"
                )
                
                if selected_dirs:
                    # å‰Šé™¤å¯¾è±¡ã®è©³ç´°æƒ…å ±
                    total_size_to_delete = sum(
                        float(info['ã‚µã‚¤ã‚º(MB)']) for info in dir_info 
                        if info['ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå'] in selected_dirs
                    )
                    total_files_to_delete = sum(
                        info['ãƒ•ã‚¡ã‚¤ãƒ«æ•°'] for info in dir_info 
                        if info['ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå'] in selected_dirs
                    )
                    
                    st.warning(f"âš ï¸ å‰Šé™¤äºˆå®š: {len(selected_dirs)}å€‹ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€{total_files_to_delete}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã€{total_size_to_delete:.1f}MB")
                    
                    # ç¢ºèªãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹
                    confirm_delete = st.checkbox(
                        "ä¸Šè¨˜ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å®Œå…¨ã«å‰Šé™¤ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™",
                        key="confirm_dataset_delete"
                    )
                    
                    if confirm_delete:
                        if st.button("ğŸ—‘ï¸ é¸æŠã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤", type="secondary"):
                            deleted_count = 0
                            deleted_size = 0
                            
                            with st.spinner("ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤ä¸­..."):
                                for dir_name in selected_dirs:
                                    dir_path = processed_data_dir / dir_name
                                    if dir_path.exists():
                                        try:
                                            # ã‚µã‚¤ã‚ºã‚’è¨˜éŒ²ã—ã¦ã‹ã‚‰å‰Šé™¤
                                            size_mb = float(next(
                                                info['ã‚µã‚¤ã‚º(MB)'] for info in dir_info 
                                                if info['ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå'] == dir_name
                                            ))
                                            
                                            shutil.rmtree(dir_path)
                                            deleted_count += 1
                                            deleted_size += size_mb
                                            
                                        except Exception as e:
                                            st.error(f"âŒ {dir_name} ã®å‰Šé™¤ã«å¤±æ•—: {e}")
                            
                            if deleted_count > 0:
                                st.success(f"âœ… {deleted_count}å€‹ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤ã—ã¾ã—ãŸï¼ˆ{deleted_size:.1f}MBå‰Šé™¤ï¼‰")
                                st.rerun()
                            else:
                                st.error("âŒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ")
        else:
            st.info("ğŸ“ å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯ã‚ã‚Šã¾ã›ã‚“")
    else:
        st.info("ğŸ“ data/processedãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“")
    
    st.divider()
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠã‚ªãƒ—ã‚·ãƒ§ãƒ³
    st.subheader("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ")
    
    # data/templatesãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
    templates_dir = Path("./data/templates")
    sample_files = []
    if templates_dir.exists():
        for ext in ['*.csv', '*.json', '*.jsonl', '*.txt']:
            sample_files.extend(templates_dir.glob(ext))
    
    tab1, tab2 = st.tabs(["ğŸ“‚ ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«", "ğŸ“¤ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"])
    
    selected_file_path = None
    
    with tab1:
        if sample_files:
            st.write("data/templatesãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«:")
            sample_file_names = [str(f.name) for f in sample_files]
            selected_sample = st.selectbox(
                "ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ:",
                options=[""] + sample_file_names,
                help="data/templatesãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰é¸æŠ"
            )
            
            if selected_sample:
                selected_file_path = str(templates_dir / selected_sample)
                st.success(f"é¸æŠã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«: {selected_sample}")
        else:
            st.info("data/templatesãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    with tab2:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        uploaded_file = st.file_uploader(
            "ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
            type=['csv', 'json', 'jsonl', 'txt'],
            help="CSVã€JSONã€JSONLã€TXTãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œ"
        )
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¾ãŸã¯ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠï¼‰
    processed_file_path = None
    
    if uploaded_file is not None:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        temp_path = f"./data/raw/{uploaded_file.name}"
        os.makedirs("./data/raw", exist_ok=True)
        
        with open(temp_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        st.success(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: {uploaded_file.name}")
        processed_file_path = temp_path
        
    elif selected_file_path:
        processed_file_path = selected_file_path
    
    if processed_file_path:
        # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
        try:
            processor = DatasetProcessor()
            df = processor.load_dataset(processed_file_path)
            
            st.subheader("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
            preview_info = processor.get_preview(df)
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("è¡Œæ•°", preview_info['shape'][0])
                st.metric("åˆ—æ•°", preview_info['shape'][1])
            
            with col2:
                st.write("**ã‚«ãƒ©ãƒ æƒ…å ±**")
                for col, dtype in preview_info['dtypes'].items():
                    st.write(f"â€¢ {col}: {dtype}")
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
            st.write("**ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿**")
            st.dataframe(df.head(), use_container_width=True)
            
            # å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³
            st.subheader("âš™ï¸ å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
            
            col1, col2 = st.columns(2)
            
            with col1:
                normalize_text = st.checkbox("æ—¥æœ¬èªæ­£è¦åŒ–", value=True)
                remove_duplicates = st.checkbox("é‡è¤‡å‰Šé™¤", value=True)
                task_type = st.selectbox(
                    "ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—",
                    options=['instruction', 'chat', 'qa', 'custom'],
                    format_func=lambda x: {
                        'instruction': 'æŒ‡ç¤ºå®Ÿè¡Œ',
                        'chat': 'ãƒãƒ£ãƒƒãƒˆ',
                        'qa': 'è³ªå•å¿œç­”',
                        'custom': 'ã‚«ã‚¹ã‚¿ãƒ '
                    }[x]
                )
            
            with col2:
                train_ratio = st.slider("è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ¯”ç‡", 0.5, 0.9, 0.8)
                min_length = st.number_input("æœ€å°æ–‡å­—æ•°", 10, 1000, 50)
                output_format = st.selectbox("å‡ºåŠ›å½¢å¼", ['jsonl', 'json', 'csv'])
            
            # ã‚«ã‚¹ã‚¿ãƒ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
            custom_template = None
            if task_type == 'custom':
                st.subheader("ğŸ“ ã‚«ã‚¹ã‚¿ãƒ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ")
                custom_template = st.text_area(
                    "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆ{column_name}ã§åˆ—ã‚’å‚ç…§ï¼‰",
                    value="### æŒ‡ç¤º:\n{instruction}\n\n### å›ç­”:\n{output}",
                    height=100
                )
            
            # å‡¦ç†å®Ÿè¡Œãƒœã‚¿ãƒ³
            if st.button("ğŸš€ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡¦ç†é–‹å§‹", type="primary"):
                with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡¦ç†ä¸­..."):
                    config = {
                        'train_split': train_ratio,
                        'val_split': (1 - train_ratio) * 0.5,
                        'test_split': (1 - train_ratio) * 0.5,
                        'min_text_length': min_length,
                        'remove_duplicates': remove_duplicates,
                        'normalize_japanese': normalize_text
                    }
                    
                    processor_with_config = DatasetProcessor(config)
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ‹¡å¼µå­ã‚’é™¤ã„ãŸéƒ¨åˆ†ã‚’å–å¾—
                    base_filename = Path(processed_file_path).stem
                    output_dir = f"./data/processed/{base_filename}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                    
                    result = processor_with_config.process_dataset(
                        processed_file_path,
                        output_dir,
                        task_type,
                        custom_template,
                        output_format
                    )
                    
                    if result['success']:
                        st.success("âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡¦ç†å®Œäº†ï¼")
                        
                        # çµæœè¡¨ç¤º
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("å…ƒãƒ‡ãƒ¼ã‚¿", f"{result['original_rows']} è¡Œ")
                        with col2:
                            st.metric("å‡¦ç†å¾Œ", f"{result['cleaned_rows']} è¡Œ")
                        with col3:
                            st.metric("ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¾Œ", f"{result['formatted_items']} ä»¶")
                        
                        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºã®æ¨å¥¨è¡¨ç¤º
                        train_count = result['splits']['train']
                        if train_count > 20:
                            st.warning("âš ï¸ **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæœ€é©åŒ–æ¨å¥¨**")
                            st.markdown("""
                            **ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ã®ãŸã‚ã®æ¨å¥¨äº‹é …:**
                            - ğŸ“Š **æœ€é©ã‚µã‚¤ã‚º**: 10-20ä»¶ãŒåŠ¹æœçš„
                            - ğŸ¯ **ç¾åœ¨ã®ã‚µã‚¤ã‚º**: {}ä»¶ï¼ˆã‚„ã‚„å¤šã‚ï¼‰
                            - ğŸ’¡ **æ¨å¥¨**: æœ€ã‚‚é‡è¦ãª10-15ä»¶ã«çµã‚Šè¾¼ã‚€ã¨å­¦ç¿’åŠ¹ç‡ãŒå‘ä¸Šã—ã¾ã™
                            - âœ¨ **åˆ©ç‚¹**: å­¦ç¿’ãŒæ·±ãã€ç‰¹å®šæƒ…å ±ã¸ã®å›ç­”ç²¾åº¦ãŒå‘ä¸Š
                            """.format(train_count))
                        elif train_count >= 10:
                            st.info("âœ… **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºè‰¯å¥½**")
                            st.markdown("""
                            **ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: {}ä»¶**
                            - ğŸ¯ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«æœ€é©ãªã‚µã‚¤ã‚ºã§ã™
                            - ğŸ“ˆ åŠ¹æœçš„ãªå­¦ç¿’ãŒæœŸå¾…ã§ãã¾ã™
                            """.format(train_count))
                        elif train_count >= 5:
                            st.info("â„¹ï¸ **å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**")
                            st.markdown("""
                            **ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: {}ä»¶**
                            - ğŸ“ å°‘æ•°ç²¾é‹­ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™
                            - ğŸš€ é«˜ã„å­¦ç¿’ç‡ã¨å¤šãã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§åŠ¹æœçš„
                            """.format(train_count))
                        else:
                            st.warning("âš ï¸ **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸è¶³**")
                            st.markdown("""
                            **ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: {}ä»¶ï¼ˆå°‘ãªã™ãã¾ã™ï¼‰**
                            - ğŸ“ˆ **æ¨å¥¨**: æœ€ä½5-10ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã—ã¦ãã ã•ã„
                            - ğŸ¯ **å“è³ªé‡è¦–**: å°‘æ•°ã§ã‚‚æ­£ç¢ºãªãƒ‡ãƒ¼ã‚¿ãŒé‡è¦ã§ã™
                            """.format(train_count))
                        
                        # åˆ†å‰²çµæœ
                        st.write("**ãƒ‡ãƒ¼ã‚¿åˆ†å‰²çµæœ**")
                        splits_df = pd.DataFrame([
                            {'åˆ†å‰²': 'è¨“ç·´', 'ä»¶æ•°': result['splits']['train']},
                            {'åˆ†å‰²': 'æ¤œè¨¼', 'ä»¶æ•°': result['splits']['val']},
                            {'åˆ†å‰²': 'ãƒ†ã‚¹ãƒˆ', 'ä»¶æ•°': result['splits']['test']}
                        ])
                        st.dataframe(splits_df, use_container_width=True)
                        
                        # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¨å¥¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                        if train_count <= 10:
                            st.info("ğŸ¯ **æ¨å¥¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šï¼ˆå°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ï¼‰**")
                            recommended_col1, recommended_col2 = st.columns(2)
                            with recommended_col1:
                                st.markdown("""
                                **åŸºæœ¬è¨­å®š:**
                                - ã‚¨ãƒãƒƒã‚¯æ•°: 5-8
                                - å­¦ç¿’ç‡: 1e-4 ï½ 2e-4
                                - ãƒãƒƒãƒã‚µã‚¤ã‚º: 1-2
                                """)
                            with recommended_col2:
                                st.markdown("""
                                **æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ:**
                                - ã‚ˆã‚Šæ·±ã„å­¦ç¿’
                                - ç‰¹å®šæƒ…å ±ã¸ã®æ­£ç¢ºãªå›ç­”
                                - éå­¦ç¿’ãƒªã‚¹ã‚¯ã®è»½æ¸›
                                """)
                        
                        # å‡ºåŠ›ãƒ‘ã‚¹è¡¨ç¤º
                        st.info(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
                        
                    else:
                        st.error(f"âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {result['error']}")
                        
        except Exception as e:
            st.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")


def training_page():
    """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒšãƒ¼ã‚¸"""
    st.title("ğŸš€ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°")
    
    # è¨­å®šèª­ã¿è¾¼ã¿
    config, models_config = load_config()
    
    # ãƒ¢ãƒ‡ãƒ«é¸æŠ
    st.subheader("ğŸ¤– ãƒ¢ãƒ‡ãƒ«é¸æŠ")
    
    model_options = {}
    default_model = None
    if 'base_models' in models_config:
        for key, model_info in models_config['base_models'].items():
            model_options[model_info['name']] = model_info['display_name']
            # Gemma2:2bã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«è¨­å®š
            if key == 'gemma2-2b':
                default_model = model_info['name']
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
    default_index = 0
    if default_model and default_model in model_options:
        default_index = list(model_options.keys()).index(default_model)
    
    selected_model = st.selectbox(
        "ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«",
        options=list(model_options.keys()),
        index=default_index,
        format_func=lambda x: model_options.get(x, x),
        help="æ¨å¥¨: Gemma 2 2B Instruct (è»½é‡ã§é«˜é€Ÿ)"
    )
    
    if selected_model and selected_model in [info['name'] for info in models_config.get('base_models', {}).values()]:
        model_info = next(info for info in models_config['base_models'].values() if info['name'] == selected_model)
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º", f"{model_info['size_gb']} GB")
        with col2:
            st.metric("ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·", f"{model_info['context_length']:,}")
        with col3:
            st.metric("æ¨å¥¨ãƒãƒƒãƒã‚µã‚¤ã‚º", model_info['recommended_batch_size'])
        
        st.info(f"ğŸ“ {model_info['description']}")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠ
    st.subheader("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠ")
    
    processed_data_dir = Path("./data/processed")
    st.info(f"ğŸ“ å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¿å­˜å ´æ‰€: `{processed_data_dir.resolve()}`")
    
    if processed_data_dir.exists():
        dataset_dirs = [d for d in processed_data_dir.iterdir() if d.is_dir()]
        
        if dataset_dirs:
            selected_dataset_dir = st.selectbox(
                "å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
                options=dataset_dirs,
                format_func=lambda x: x.name,
                help="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ã§ä½œæˆã•ã‚ŒãŸå‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰é¸æŠ"
            )
            
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±è¡¨ç¤º
            if selected_dataset_dir:
                st.write(f"**é¸æŠã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª**: `{selected_dataset_dir}`")
                
                train_file = selected_dataset_dir / "train.jsonl"
                val_file = selected_dataset_dir / "val.jsonl"
                test_file = selected_dataset_dir / "test.jsonl"
                
                col1, col2, col3 = st.columns(3)
                
                if train_file.exists():
                    with open(train_file, 'r') as f:
                        train_count = sum(1 for _ in f)
                    with col1:
                        st.metric("ğŸ“Š è¨“ç·´ãƒ‡ãƒ¼ã‚¿", f"{train_count:,} ä»¶")
                
                if val_file.exists():
                    with open(val_file, 'r') as f:
                        val_count = sum(1 for _ in f)
                    with col2:
                        st.metric("ğŸ“Š æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿", f"{val_count:,} ä»¶")
                
                if test_file.exists():
                    with open(test_file, 'r') as f:
                        test_count = sum(1 for _ in f)
                    with col3:
                        st.metric("ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿", f"{test_count:,} ä»¶")
        else:
            st.warning("å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚")
            selected_dataset_dir = None
    else:
        st.warning("ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        selected_dataset_dir = None
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«é…ç½®æ‰‹é †
    st.subheader("ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«é…ç½®")
    
    with st.expander("ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®æº–å‚™æ‰‹é †", expanded=True):
        st.info("""
        **ã“ã®ã‚¢ãƒ—ãƒªã¯ãƒ­ãƒ¼ã‚«ãƒ«ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ä»¥ä¸‹ã®æ‰‹é †ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é…ç½®ã—ã¦ãã ã•ã„ï¼š**
        """)
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«é…ç½®çŠ¶æ³ã®ç¢ºèª
        gemma_path = Path("./models/gemma-2-2b-it")
        elyza_path = Path("./models/Llama-3-ELYZA-JP-8B")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**ğŸ¤– Gemma 2 2B Instruct**")
            if gemma_path.exists():
                required_files = ["config.json", "model.safetensors.index.json"]
                safetensors_files = list(gemma_path.glob("model-*.safetensors"))
                tokenizer_files = [f for f in ["tokenizer.json", "tokenizer.model"] if (gemma_path / f).exists()]
                
                all_required_exist = all((gemma_path / f).exists() for f in required_files)
                has_tokenizer = len(tokenizer_files) > 0
                has_model_files = len(safetensors_files) > 0
                
                if all_required_exist and has_tokenizer and has_model_files:
                    st.success("âœ… ãƒ•ã‚¡ã‚¤ãƒ«é…ç½®å®Œäº†")
                    st.write(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(safetensors_files)}")
                else:
                    st.error("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¸è¶³")
                    missing = []
                    if not all_required_exist:
                        missing.extend([f for f in required_files if not (gemma_path / f).exists()])
                    if not has_tokenizer:
                        missing.append("tokenizer files")
                    if not has_model_files:
                        missing.append("model-*.safetensors")
                    st.write(f"ä¸è¶³ãƒ•ã‚¡ã‚¤ãƒ«: {', '.join(missing)}")
            else:
                st.warning("âš ï¸ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãªã—")
                st.code(f"é…ç½®å…ˆ: {gemma_path.absolute()}")
        
        with col2:
            st.write("**ğŸ—¾ ELYZA Japanese 8B**")
            if elyza_path.exists():
                st.success("âœ… ãƒ•ã‚¡ã‚¤ãƒ«é…ç½®å®Œäº†")
            else:
                st.warning("âš ï¸ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãªã—") 
                st.code(f"é…ç½®å…ˆ: {elyza_path.absolute()}")
        
        st.markdown("""
        ### ğŸ“¥ ãƒ•ã‚¡ã‚¤ãƒ«å…¥æ‰‹æ–¹æ³•:
        
        #### **Gemma 2 2B Instruct**:
        1. [HuggingFace](https://huggingface.co/google/gemma-2-2b-it) ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        2. ä»¥ä¸‹ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®: `./models/gemma-2-2b-it/`
        3. å¿…è¦ãƒ•ã‚¡ã‚¤ãƒ«:
           - `config.json`
           - `model-*.safetensors` (è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«)
           - `model.safetensors.index.json`  
           - `tokenizer.json` ã¾ãŸã¯ `tokenizer.model`
        
        #### **ELYZA Japanese 8B**:
        1. [HuggingFace](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B) ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        2. ä»¥ä¸‹ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®: `./models/Llama-3-ELYZA-JP-8B/`
        """)
    
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    st.subheader("âš™ï¸ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š")
    
    # ã‚¹ãƒãƒ¼ãƒˆæ¨å¥¨æ©Ÿèƒ½
    if selected_dataset_dir:
        with st.expander("ğŸ§  AIæ¨å¥¨è¨­å®š", expanded=True):
            try:
                from src.smart_recommender import SmartParameterRecommender
                recommender = SmartParameterRecommender()
                
                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‘ã‚¹ã‚’å–å¾—
                train_file_path = str(selected_dataset_dir / "train.jsonl")
                
                # æ¨å¥¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
                recommendation = recommender.recommend_training_parameters(train_file_path)
                
                col_rec1, col_rec2 = st.columns([2, 1])
                with col_rec1:
                    st.info(f"ğŸ’¡ **æ¨å¥¨ç†ç”±**: {recommendation['rationale']}")
                    st.write(f"â±ï¸ **äºˆæƒ³å®Ÿè¡Œæ™‚é–“**: {recommendation['estimated_time_minutes']:.1f}åˆ†")
                    st.write(f"ğŸ”„ **äºˆæƒ³ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°**: {recommendation['estimated_iterations']:,}å›")
                with col_rec2:
                    apply_ai_recommendations = st.checkbox("AIæ¨å¥¨è¨­å®šã‚’é©ç”¨", value=True, key="training_ai_rec")
                
                # ä¿¡é ¼åº¦è¡¨ç¤º
                confidence_color = {"high": "ğŸŸ¢", "medium": "ğŸŸ¡", "low": "ğŸ”´"}
                st.write(f"**ä¿¡é ¼åº¦**: {confidence_color.get(recommendation['confidence_level'], 'â“')} {recommendation['confidence_level']}")
                
            except Exception as e:
                st.warning(f"AIæ¨å¥¨æ©Ÿèƒ½ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
                apply_ai_recommendations = False
                recommendation = None
    else:
        apply_ai_recommendations = False
        recommendation = None
    
    with st.expander("åŸºæœ¬è¨­å®š", expanded=True):
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®šï¼ˆAIæ¨å¥¨ãŒæœ‰åŠ¹ãªå ´åˆã¯ãã‚Œã‚’ä½¿ç”¨ï¼‰
        default_batch_size = 1
        default_learning_rate = 5e-5
        default_epochs = 3
        default_lora_rank = 16
        default_lora_alpha = 32
        default_lora_dropout = 0.1
        
        if apply_ai_recommendations and recommendation:
            params = recommendation.get('parameters', {})
            default_batch_size = params.get('batch_size', 1)
            default_learning_rate = params.get('learning_rate', 5e-5)
            default_epochs = params.get('num_epochs', 3)
            default_lora_rank = params.get('lora_rank', 16)
            default_lora_alpha = params.get('lora_alpha', 32)
            default_lora_dropout = params.get('lora_dropout', 0.1)
        
        col1, col2 = st.columns(2)
        
        with col1:
            batch_size = st.selectbox("ãƒãƒƒãƒã‚µã‚¤ã‚º", [1, 2, 4], 
                                      index=[1, 2, 4].index(default_batch_size) if default_batch_size in [1, 2, 4] else 0)
            
            lr_options = [1e-5, 2e-5, 5e-5, 1e-4, 2e-4]
            lr_index = 2  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§5e-5
            if default_learning_rate in lr_options:
                lr_index = lr_options.index(default_learning_rate)
            elif default_learning_rate > 1e-4:
                lr_index = 4  # 2e-4ã‚’é¸æŠ
            elif default_learning_rate > 5e-5:
                lr_index = 3  # 1e-4ã‚’é¸æŠ
                
            learning_rate = st.select_slider(
                "å­¦ç¿’ç‡",
                options=lr_options,
                value=lr_options[lr_index],
                format_func=lambda x: f"{x:.0e}"
            )
            num_epochs = st.slider("ã‚¨ãƒãƒƒã‚¯æ•°", 1, 15, min(default_epochs, 15))
        
        with col2:
            lora_rank = st.slider("LoRA Rank", 4, 64, min(default_lora_rank, 64))
            lora_alpha = st.slider("LoRA Alpha", 8, 256, min(default_lora_alpha, 256))
            lora_dropout = st.slider("LoRA Dropout", 0.0, 0.3, default_lora_dropout)
    
    with st.expander("è©³ç´°è¨­å®š"):
        col1, col2 = st.columns(2)
        
        with col1:
            gradient_accumulation_steps = st.number_input("å‹¾é…è“„ç©ã‚¹ãƒ†ãƒƒãƒ—", 1, 32, 8)
            warmup_steps = st.number_input("ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—", 0, 1000, 100)
            weight_decay = st.number_input("é‡ã¿æ¸›è¡°", 0.0, 0.1, 0.01, format="%.3f")
        
        with col2:
            save_steps = st.number_input("ä¿å­˜é–“éš”", 100, 2000, 500)
            eval_steps = st.number_input("è©•ä¾¡é–“éš”", 100, 2000, 500)
            early_stopping_patience = st.number_input("æ—©æœŸåœæ­¢å¾…æ©Ÿ", 1, 10, 3)
    
    # ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯
    if selected_model:
        memory_monitor = MemoryMonitor()
        can_run, message = memory_monitor.can_run_training(selected_model, batch_size)
        
        if can_run:
            st.success(f"âœ… {message}")
        else:
            st.error(f"âŒ {message}")
            suggested_batch_size = memory_monitor.suggest_batch_size(selected_model)
            st.info(f"ğŸ’¡ æ¨å¥¨ãƒãƒƒãƒã‚µã‚¤ã‚º: {suggested_batch_size}")
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
    st.subheader("ğŸš€ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ")
    
    if selected_dataset_dir and selected_model:
        train_file_path = str(selected_dataset_dir / "train.jsonl")
        
        if st.button("ğŸš€ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹", type="primary"):
            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š
            training_config = {
                'batch_size': batch_size,
                'learning_rate': learning_rate,
                'num_epochs': num_epochs,
                'lora_rank': lora_rank,
                'lora_alpha': lora_alpha,
                'lora_dropout': lora_dropout,
                'gradient_accumulation_steps': gradient_accumulation_steps,
                'warmup_steps': warmup_steps,
                'weight_decay': weight_decay,
                'save_steps': save_steps,
                'eval_steps': eval_steps,
                'early_stopping_patience': early_stopping_patience
            }
            
            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’åˆæœŸåŒ–
            trainer = TrainingManager(training_config)
            st.session_state['training_manager'] = trainer
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çŠ¶æ³ã‚’ç®¡ç†
            if 'training_progress' not in st.session_state:
                st.session_state['training_progress'] = 0
            if 'training_status' not in st.session_state:
                st.session_state['training_status'] = "æº–å‚™ä¸­..."
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã¨ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º  
            progress_container = st.container()
            with progress_container:
                st.markdown("### ğŸš€ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€²è¡ŒçŠ¶æ³")
                progress_bar = st.progress(st.session_state['training_progress'])
                progress_col1, progress_col2 = st.columns(2)
                with progress_col1:
                    st.metric("ğŸ”„ é€²è¡ŒçŠ¶æ³", f"{st.session_state['training_progress']*100:.1f}%")
                with progress_col2:
                    if 'training_current_loss' not in st.session_state:
                        st.session_state['training_current_loss'] = 'N/A'
                    st.metric("ğŸ“Š ç¾åœ¨ã®Loss", st.session_state['training_current_loss'])
                
                status_text = st.text_area("ğŸ“ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹", st.session_state['training_status'], height=100, disabled=True)
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’æ›´æ–°ã™ã‚‹é–¢æ•°ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ï¼‰
            def update_progress(progress):
                st.session_state['training_progress'] = progress
            
            def update_status(status):
                st.session_state['training_status'] = status
            
            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹ï¼ˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ç„¡ã—ã§å®Ÿè¡Œï¼‰
            try:
                experiment_id = trainer.start_training(
                    selected_model,
                    train_file_path,
                    progress_callback=None,  # ç„¡åŠ¹åŒ–
                    status_callback=None     # ç„¡åŠ¹åŒ–
                )
                
                st.session_state['current_experiment_id'] = experiment_id
                st.success(f"âœ… ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã—ãŸï¼ˆå®Ÿé¨“ID: {experiment_id}ï¼‰")
                
            except Exception as e:
                st.error(f"âŒ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ç¾åœ¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çŠ¶æ³ï¼ˆæ”¹å–„ç‰ˆï¼‰
    st.subheader("ğŸ” ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çŠ¶æ³ç¢ºèª")
    
    # æœ€æ–°ã®å®Ÿé¨“ã‚’è‡ªå‹•å–å¾—
    tracker = ExperimentTracker()
    experiments = tracker.list_experiments()
    
    if experiments:
        latest_experiment = experiments[0]
        experiment_id = latest_experiment['id']
        
        # å®Ÿé¨“çŠ¶æ³è¡¨ç¤º
        col1, col2, col3 = st.columns(3)
        with col1:
            status = latest_experiment.get('status', 'unknown')
            status_emoji = {"running": "ğŸ”„", "completed": "âœ…", "failed": "âŒ"}
            st.metric("ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹", f"{status_emoji.get(status, 'â“')} {status}")
            
        with col2:
            duration = latest_experiment.get('duration_seconds')
            if duration:
                duration_str = f"{duration:.1f}ç§’"
            elif status == 'running':
                import time
                start_time = latest_experiment.get('started_at')
                if start_time:
                    from datetime import datetime
                    start = datetime.fromisoformat(start_time.replace('Z', '+00:00'))
                    now = datetime.now(start.tzinfo)
                    duration = (now - start).total_seconds()
                    duration_str = f"{duration:.0f}ç§’çµŒé"
                else:
                    duration_str = "å®Ÿè¡Œä¸­"
            else:
                duration_str = "N/A"
            st.metric("å®Ÿè¡Œæ™‚é–“", duration_str)
            
        with col3:
            st.metric("å®Ÿé¨“ID", experiment_id[:8])
        
        # é€²è¡ŒçŠ¶æ³ãƒãƒ¼ï¼ˆæ¨å®šï¼‰
        if status == 'running':
            # å®Ÿè¡Œæ™‚é–“ãƒ™ãƒ¼ã‚¹ã§é€²è¡Œç‡ã‚’æ¨å®š
            estimated_total_time = 120  # 2åˆ†ã¨æ¨å®š
            if duration:
                progress = min(duration / estimated_total_time, 0.95)  # æœ€å¤§95%ã¾ã§
            else:
                progress = 0.1
            
            st.progress(progress, text=f"é€²è¡Œä¸­... ({progress*100:.0f}%)")
            
            # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°ãƒœã‚¿ãƒ³
            col_refresh1, col_refresh2 = st.columns(2)
            with col_refresh1:
                if st.button("ğŸ”„ çŠ¶æ³æ›´æ–°", type="secondary"):
                    st.rerun()
            with col_refresh2:
                # è‡ªå‹•ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ï¼ˆ5ç§’é–“éš”ï¼‰
                if st.button("â¸ï¸ è‡ªå‹•æ›´æ–°åœæ­¢", help="5ç§’ã”ã¨ã®è‡ªå‹•æ›´æ–°ã‚’åœæ­¢"):
                    st.session_state['auto_refresh'] = False
                    
            # è‡ªå‹•ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥æ©Ÿèƒ½
            if st.session_state.get('auto_refresh', True):
                import time
                time.sleep(5)
                st.rerun()
                
        elif status == 'completed':
            st.progress(1.0, text="å®Œäº† (100%)")
            
            # å®Œäº†æ™‚ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
            col_action1, col_action2, col_action3 = st.columns(3)
            with col_action1:
                if st.button("ğŸ“ˆ å®Ÿé¨“è©³ç´°"):
                    st.switch_page("experiments")
            with col_action2:
                if st.button("ğŸ“¦ é‡å­åŒ–ã¸"):
                    st.switch_page("quantization")  
            with col_action3:
                if st.button("ğŸ¤– Ollamaçµ±åˆ"):
                    st.switch_page("Ollamaçµ±åˆ")
                    
            # æœ€çµ‚ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
            final_metrics = latest_experiment.get('final_metrics', {})
            if final_metrics:
                st.subheader("ğŸ“Š æœ€çµ‚çµæœ")
                metric_col1, metric_col2 = st.columns(2)
                with metric_col1:
                    if 'final_loss' in final_metrics:
                        st.metric("æœ€çµ‚Loss", f"{final_metrics['final_loss']:.3f}")
                with metric_col2:
                    if 'perplexity' in final_metrics:
                        st.metric("Perplexity", f"{final_metrics['perplexity']:.1f}")
                        
        elif status == 'failed':
            st.progress(0.0, text="å¤±æ•—")
            st.error(f"âŒ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå¤±æ•—ã—ã¾ã—ãŸ: {latest_experiment.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}")
            
    else:
        st.info("ğŸ“ ã¾ã ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç®¡ç†ã®çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆ
    if st.session_state.get('training_manager'):
        status = st.session_state['training_manager'].get_training_status()
        if not status['is_training'] and experiments and experiments[0].get('status') == 'completed':
            # å®Œäº†çŠ¶æ…‹ã‚’æ­£ã—ãèªè­˜ã•ã›ã‚‹
            st.session_state['training_manager'].is_training = False
            st.session_state['training_manager'].current_experiment_id = None


def quantization_page():
    """é‡å­åŒ–ãƒšãƒ¼ã‚¸"""
    st.title("ğŸ“¦ é‡å­åŒ–")
    
    quantizer = ModelQuantizer()
    
    # ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ç®¡ç†ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    st.subheader("ğŸ’¾ ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ç®¡ç†")
    
    # ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡è¡¨ç¤º
    disk_info = get_disk_usage()
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("ä½¿ç”¨å®¹é‡", f"{disk_info['used_gb']:.1f} GB")
    with col2:
        st.metric("ç©ºãå®¹é‡", f"{disk_info['free_gb']:.1f} GB")
    with col3:
        capacity_color = "ğŸ”´" if disk_info['usage_percent'] > 95 else "ğŸŸ¡" if disk_info['usage_percent'] > 85 else "ğŸŸ¢"
        st.metric("ä½¿ç”¨ç‡", f"{capacity_color} {disk_info['usage_percent']:.1f}%")
    
    # å®¹é‡ä¸è¶³è­¦å‘Š
    if disk_info['free_gb'] < 5:
        st.warning(f"âš ï¸ ç©ºãå®¹é‡ãŒ{disk_info['free_gb']:.1f}GBã§ã™ã€‚é‡å­åŒ–ã«ã¯æœ€ä½5GBå¿…è¦ã§ã™ã€‚")
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æ©Ÿèƒ½
        with st.expander("ğŸ§¹ ãƒ‡ã‚£ã‚¹ã‚¯ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—", expanded=True):
            st.markdown("""
            **ä»¥ä¸‹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œã—ã¦ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ã‚’ç¢ºä¿ã§ãã¾ã™ï¼š**
            """)
            
            cleanup_col1, cleanup_col2 = st.columns(2)
            
            with cleanup_col1:
                if st.button("ğŸ³ Dockerã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—", help="æœªä½¿ç”¨ã®Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ãƒ»ã‚³ãƒ³ãƒ†ãƒŠãƒ»ãƒœãƒªãƒ¥ãƒ¼ãƒ ã‚’å‰Šé™¤"):
                    cleanup_docker()
                
                if st.button("ğŸ—‚ï¸ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤", help="ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãªã©ã‚’å‰Šé™¤"):
                    cleanup_temp_files()
            
            with cleanup_col2:
                if st.button("ğŸ“¦ é‡å­åŒ–ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†", help="å¤ã„é‡å­åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚„å¤±æ•—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤"):
                    cleanup_quantization_files()
                
                if st.button("ğŸ”„ å®¹é‡å†ç¢ºèª", help="ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡ã‚’æœ€æ–°çŠ¶æ…‹ã«æ›´æ–°"):
                    st.rerun()
    
    st.divider()
    
    # llama.cppçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯
    st.subheader("ğŸ”§ llama.cpp çŠ¶æ…‹")
    
    if quantizer.check_llama_cpp():
        st.success("âœ… llama.cpp ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
    else:
        st.error("âŒ llama.cpp ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        with st.expander("ğŸ“‹ è§£æ±ºæ–¹æ³•ï¼ˆåˆå¿ƒè€…å‘ã‘ï¼‰", expanded=True):
            st.warning("**é‡å­åŒ–ã«ã¯llama.cppã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå¿…è¦ã§ã™**")
            
            st.markdown("""
            ### ğŸ› ï¸ è‡ªå‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †:
            
            #### **æ–¹æ³•1: è‡ªå‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæ¨å¥¨ï¼‰**
            ä»¥ä¸‹ã®ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã¨è‡ªå‹•ã§llama.cppã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã—ã¾ã™ï¼š
            """)
            
            col1, col2 = st.columns([1, 2])
            with col1:
                if st.button("ğŸ”§ è‡ªå‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Ÿè¡Œ", type="primary"):
                    with st.spinner("llama.cppã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­..."):
                        try:
                            # è‡ªå‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ
                            import subprocess
                            result = subprocess.run(
                                ["bash", "./setup.sh"], 
                                capture_output=True, 
                                text=True,
                                timeout=600  # 10åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                            )
                            
                            if result.returncode == 0:
                                st.success("âœ… ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†ï¼ãƒšãƒ¼ã‚¸ã‚’æ›´æ–°ã—ã¦ãã ã•ã„")
                                st.rerun()
                            else:
                                st.error(f"âŒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—: {result.stderr}")
                                
                        except subprocess.TimeoutExpired:
                            st.error("âŒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸï¼ˆ10åˆ†ï¼‰")
                        except Exception as e:
                            st.error(f"âŒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            
            with col2:
                if st.button("ğŸ”„ çŠ¶æ…‹ã‚’å†ç¢ºèª"):
                    st.rerun()
            
            st.markdown("""
            #### **æ–¹æ³•2: æ‰‹å‹•å®Ÿè¡Œï¼ˆä¸Šç´šè€…å‘ã‘ï¼‰**
            ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š
            ```bash
            cd /Users/matsbaccano/Projects/clone/mlx-finetuning
            ./setup.sh
            ```
            
            #### **ğŸ“‹ setup.shã®å‡¦ç†å†…å®¹:**
            - Homebrewã®ç¢ºèªãƒ»ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
            - Minicondaã®ç¢ºèªãƒ»ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«  
            - llama.cppã®ã‚¯ãƒ­ãƒ¼ãƒ³ã¨ãƒ“ãƒ«ãƒ‰
            - Ollamaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
            - å¿…è¦ãªä¾å­˜é–¢ä¿‚ã®è¨­å®š
            
            #### **â±ï¸ æ‰€è¦æ™‚é–“:**
            åˆå›ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—: ç´„5-15åˆ†ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é€Ÿåº¦ã«ã‚ˆã‚‹ï¼‰
            """)
        
        return
    
    # é‡å­åŒ–æ–¹æ³•ã®èª¬æ˜
    st.subheader("ğŸ“Š é‡å­åŒ–æ–¹æ³•")
    
    quant_info = quantizer.get_quantization_info()
    methods_df = pd.DataFrame([
        {
            'æ–¹æ³•': method,
            'èª¬æ˜': info['description'],
            'ã‚µã‚¤ã‚ºæ¯”': f"{info['size_ratio']*100:.0f}%",
            'å“è³ª': info['quality']
        }
        for method, info in quant_info['available_methods'].items()
    ])
    
    st.dataframe(methods_df, use_container_width=True)
    
    # ãƒ¢ãƒ‡ãƒ«é¸æŠ
    st.subheader("ğŸ¤– ãƒ¢ãƒ‡ãƒ«é¸æŠ")
    
    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œç´¢
    finetuned_dir = Path("./models/finetuned")
    available_models = []
    
    if finetuned_dir.exists():
        for model_dir in finetuned_dir.iterdir():
            if model_dir.is_dir():
                # MLXãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœã‚’æ¤œç´¢
                adapters_file = model_dir / "adapters.safetensors"
                mlx_model_dirs = list(model_dir.glob("mlx_model_*"))
                
                if adapters_file.exists() and mlx_model_dirs:
                    # MLXãƒ¢ãƒ‡ãƒ«ã¨LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®çµ„ã¿åˆã‚ã›
                    mlx_model_path = mlx_model_dirs[0]  # æœ€åˆã®MLXãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨
                    model_info = {
                        'path': str(mlx_model_path),
                        'adapters': str(adapters_file),
                        'experiment_id': model_dir.name,
                        'display_name': f'Gemma-2-2b-it + LoRA ({model_dir.name[:8]})'
                    }
                    available_models.append(model_info)
                
                # å¾“æ¥ã®final_modelãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚‚ã‚µãƒãƒ¼ãƒˆ
                final_model_path = model_dir / "final_model"
                if final_model_path.exists():
                    model_info = {
                        'path': str(final_model_path),
                        'adapters': None,
                        'experiment_id': model_dir.name,
                        'display_name': f'Traditional Model ({model_dir.name[:8]})'
                    }
                    available_models.append(model_info)
    
    if available_models:
        selected_model_info = st.selectbox(
            "ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«",
            options=available_models,
            format_func=lambda x: x['display_name']
        )
        selected_model_path = selected_model_info['path']
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’è¡¨ç¤º
        col1, col2 = st.columns(2)
        with col1:
            st.info(f"**ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«**: {selected_model_info['path']}")
        with col2:
            if selected_model_info['adapters']:
                st.info(f"**LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼**: {selected_model_info['adapters']}")
            else:
                st.info("**ãƒ¢ãƒ‡ãƒ«**: å¾“æ¥å½¢å¼")
    else:
        st.warning("ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        st.info("ä»£ã‚ã‚Šã«ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’ç›´æ¥æŒ‡å®šã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™")
        selected_model_path = st.text_input("ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹", placeholder="/path/to/model")
    
    if selected_model_path:
        # ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼
        validation_result = quantizer.validate_model_path(selected_model_path)
        
        if validation_result['is_valid']:
            st.success(f"âœ… ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼OK (ã‚µã‚¤ã‚º: {validation_result['model_size_gb']:.1f} GB)")
        else:
            for error in validation_result['errors']:
                st.error(f"âŒ {error}")
            for warning in validation_result['warnings']:
                st.warning(f"âš ï¸ {warning}")
    
    # é‡å­åŒ–è¨­å®š
    st.subheader("âš™ï¸ é‡å­åŒ–è¨­å®š")
    
    # ã‚¹ãƒãƒ¼ãƒˆæ¨å¥¨æ©Ÿèƒ½
    if selected_model_path and validation_result.get('is_valid'):
        with st.expander("ğŸ§  AIæ¨å¥¨è¨­å®š", expanded=True):
            try:
                from src.smart_recommender import SmartParameterRecommender
                recommender = SmartParameterRecommender()
                
                # ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºå–å¾—
                model_size_gb = validation_result.get('model_size_gb', 2.0)
                
                # ç”¨é€”é¸æŠUI
                use_case = st.selectbox(
                    "ç”¨é€”ã‚’é¸æŠï¼ˆæ¨å¥¨è¨­å®šã®å‚è€ƒã«ã—ã¾ã™ï¼‰",
                    options=["é«˜ç²¾åº¦é‡è¦–", "é€Ÿåº¦é‡è¦–", "ãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–", "ãƒãƒ©ãƒ³ã‚¹é‡è¦–"],
                    index=3,  # ãƒãƒ©ãƒ³ã‚¹é‡è¦–ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                    help="é¸æŠã—ãŸç”¨é€”ã«å¿œã˜ã¦æœ€é©ãªé‡å­åŒ–æ–¹æ³•ã‚’æ¨å¥¨ã—ã¾ã™"
                )
                
                # æ¨å¥¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
                recommendation = recommender.recommend_quantization_parameters(model_size_gb, use_case)
                
                col_rec1, col_rec2 = st.columns([2, 1])
                with col_rec1:
                    st.info(f"ğŸ’¡ **æ¨å¥¨**: {recommendation['method']} - {recommendation['reason']}")
                with col_rec2:
                    apply_recommendations = st.checkbox("æ¨å¥¨è¨­å®šã‚’é©ç”¨", value=True)
                
            except Exception as e:
                st.warning(f"æ¨å¥¨æ©Ÿèƒ½ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
                apply_recommendations = False
    else:
        apply_recommendations = False
    
    col1, col2 = st.columns(2)
    
    with col1:
        # æ¨å¥¨è¨­å®šãŒåˆ©ç”¨å¯èƒ½ã§é©ç”¨ã•ã‚Œã‚‹å ´åˆ
        if apply_recommendations and 'recommendation' in locals():
            default_method = recommendation['method']
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—
            method_list = list(quant_info['available_methods'].keys())
            try:
                default_index = method_list.index(default_method)
            except ValueError:
                default_index = 1  # Q5_K_Mã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        else:
            default_index = 1  # Q5_K_Mã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        
        quantization_method = st.selectbox(
            "é‡å­åŒ–æ–¹æ³•",
            options=list(quant_info['available_methods'].keys()),
            index=default_index
        )
        
        output_name = st.text_input(
            "å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å",
            value=f"model-{quantization_method.lower()}",
            help="æ‹¡å¼µå­(.gguf)ã¯è‡ªå‹•ã§è¿½åŠ ã•ã‚Œã¾ã™"
        )
    
    with col2:
        if selected_model_path and validation_result.get('is_valid'):
            input_size = validation_result['model_size_gb']
            estimated_size = quantizer.estimate_output_size(input_size, quantization_method)
            
            st.metric("å…¥åŠ›ã‚µã‚¤ã‚º", f"{input_size:.1f} GB")
            st.metric("æ¨å®šå‡ºåŠ›ã‚µã‚¤ã‚º", f"{estimated_size:.1f} GB")
            st.metric("åœ§ç¸®ç‡", f"{(estimated_size/input_size)*100:.0f}%")
    
    # é‡å­åŒ–å®Ÿè¡Œ
    if selected_model_path and st.button("ğŸš€ é‡å­åŒ–é–‹å§‹", type="primary"):
        output_dir = "./models/quantized"
        os.makedirs(output_dir, exist_ok=True)
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        def update_progress(progress):
            progress_bar.progress(progress)
        
        def update_status(status):
            status_text.text(status)
        
        with st.spinner("é‡å­åŒ–å‡¦ç†ä¸­..."):
            result = quantizer.full_quantization_pipeline(
                selected_model_path,
                output_dir,
                quantization_method,
                progress_callback=update_progress,
                status_callback=update_status
            )
            
            if result['success']:
                st.success("âœ… é‡å­åŒ–å®Œäº†ï¼")
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("GGUF ã‚µã‚¤ã‚º", f"{result.get('gguf_size_mb', 0)/1024:.1f} GB")
                with col2:
                    st.metric("é‡å­åŒ–å¾Œã‚µã‚¤ã‚º", f"{result.get('quantized_size_mb', 0)/1024:.1f} GB")
                with col3:
                    st.metric("åœ§ç¸®ç‡", f"{result.get('compression_ratio', 1)*100:.0f}%")
                
                st.info(f"ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {result['quantized_path']}")
                
                # Ollamaçµ±åˆã®ææ¡ˆ
                if st.button("ğŸ¤– Ollamaã«ç™»éŒ²"):
                    st.session_state['quantized_model_path'] = result['quantized_path']
                    st.switch_page("Ollamaçµ±åˆ")
                    
            else:
                st.error(f"âŒ é‡å­åŒ–ã‚¨ãƒ©ãƒ¼: {result['error']}")


def ollama_page():
    """Ollamaçµ±åˆãƒšãƒ¼ã‚¸"""
    st.title("ğŸ¤– Ollama çµ±åˆ")
    
    integrator = OllamaIntegrator()
    
    # OllamaçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯
    st.subheader("ğŸ”§ Ollama ã‚µãƒ¼ãƒãƒ¼çŠ¶æ…‹")
    
    status = integrator.check_ollama_status()
    
    if status['status'] == 'running':
        st.success(f"âœ… Ollama ã‚µãƒ¼ãƒãƒ¼ç¨¼åƒä¸­ ({status['url']})")
        st.info(f"ğŸ“Š ç™»éŒ²æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«: {status['models_count']} å€‹")
    elif status['status'] == 'not_running':
        st.error("âŒ Ollama ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã¾ã›ã‚“")
        if st.button("ğŸš€ Ollama ã‚µãƒ¼ãƒãƒ¼èµ·å‹•"):
            with st.spinner("ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ä¸­..."):
                if integrator.start_ollama_server():
                    st.success("âœ… ã‚µãƒ¼ãƒãƒ¼èµ·å‹•å®Œäº†ï¼")
                    st.rerun()
                else:
                    st.error("âŒ ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    else:
        st.error(f"âŒ Ollama ã‚¨ãƒ©ãƒ¼: {status['error']}")
        return
    
    # æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ä¸€è¦§
    st.subheader("ğŸ“‹ ç™»éŒ²æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«")
    
    models = integrator.list_models()
    if models:
        models_df = pd.DataFrame([
            {
                'ãƒ¢ãƒ‡ãƒ«å': model['name'],
                'ã‚µã‚¤ã‚º': f"{model.get('size', 0) / (1024**3):.1f} GB",
                'æ›´æ–°æ—¥': model.get('modified_at', 'N/A')[:19] if model.get('modified_at') else 'N/A'
            }
            for model in models
        ])
        st.dataframe(models_df, use_container_width=True)
    else:
        st.info("ç™»éŒ²æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
    
    # æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã®ç™»éŒ²
    st.subheader("â• æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ç™»éŒ²")
    
    # é‡å­åŒ–æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œç´¢
    quantized_dir = Path("./models/quantized")
    available_gguf = []
    
    if quantized_dir.exists():
        available_gguf = list(quantized_dir.glob("*.gguf"))
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰é‡å­åŒ–æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—
    if 'quantized_model_path' in st.session_state:
        available_gguf.insert(0, Path(st.session_state['quantized_model_path']))
    
    if available_gguf:
        selected_gguf = st.selectbox(
            "GGUFãƒ•ã‚¡ã‚¤ãƒ«",
            options=available_gguf,
            format_func=lambda x: x.name
        )
    else:
        st.warning("é‡å­åŒ–æ¸ˆã¿GGUFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        selected_gguf = st.text_input("GGUFãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹", placeholder="/path/to/model.gguf")
        if selected_gguf:
            selected_gguf = Path(selected_gguf)
    
    if selected_gguf:
        # ãƒ¢ãƒ‡ãƒ«åè¨­å®š
        col1, col2 = st.columns(2)
        
        with col1:
            model_name = st.text_input(
                "ãƒ¢ãƒ‡ãƒ«å",
                value=selected_gguf.stem if hasattr(selected_gguf, 'stem') else "",
                help="è‹±æ•°å­—ã€ãƒã‚¤ãƒ•ãƒ³ã€ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã®ã¿ä½¿ç”¨å¯èƒ½"
            )
            
            # ãƒ¢ãƒ‡ãƒ«åæ¤œè¨¼
            if model_name:
                validation = integrator.validate_model_name(model_name)
                if not validation['is_valid']:
                    for error in validation['errors']:
                        st.error(f"âŒ {error}")
                for warning in validation['warnings']:
                    st.warning(f"âš ï¸ {warning}")
        
        with col2:
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé¸æŠ
            templates = integrator.get_system_prompt_templates()
            template_names = list(templates.keys())
            
            selected_template = st.selectbox(
                "ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
                options=template_names,
                format_func=lambda x: templates[x]['name']
            )
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç·¨é›†
        if selected_template == 'custom':
            system_prompt = st.text_area(
                "ã‚«ã‚¹ã‚¿ãƒ ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
                height=150,
                placeholder="ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."
            )
        else:
            system_prompt = st.text_area(
                "ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
                value=templates[selected_template]['prompt'],
                height=150
            )
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        st.subheader("âš™ï¸ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š")
        
        # ã‚¹ãƒãƒ¼ãƒˆæ¨å¥¨æ©Ÿèƒ½
        with st.expander("ğŸ§  AIæ¨å¥¨è¨­å®š", expanded=True):
            try:
                from src.smart_recommender import SmartParameterRecommender
                recommender = SmartParameterRecommender()
                
                # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’å–å¾—ã‚’è©¦è¡Œ
                dataset_stats = {'total_samples': 10, 'has_specific_knowledge': True}  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                
                # å®Ÿé¨“IDã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†æçµæœã‚’å–å¾—ã™ã‚‹è©¦è¡Œ
                if selected_gguf and hasattr(selected_gguf, 'name'):
                    model_filename = selected_gguf.name
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å®Ÿé¨“IDã‚’æŠ½å‡ºï¼ˆä¾‹: mlx_model_1754767357-Q5_K_M.gguf -> 1754767357ï¼‰
                    import re
                    experiment_match = re.search(r'mlx_model_(\d+)', model_filename)
                    if experiment_match:
                        experiment_timestamp = experiment_match.group(1)
                        # å®Ÿé¨“æƒ…å ±ã‹ã‚‰å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‘ã‚¹ã‚’å–å¾—ã™ã‚‹è©¦è¡Œ
                        from src.experiment_tracker import ExperimentTracker
                        tracker = ExperimentTracker()
                        try:
                            # æœ€è¿‘ã®å®Ÿé¨“ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆæƒ…å ±ã‚’å–å¾—
                            experiments = tracker.list_experiments()
                            if experiments:
                                latest_exp = experiments[0]  # æœ€æ–°å®Ÿé¨“
                                if 'dataset_analysis' in latest_exp.get('final_metrics', {}):
                                    dataset_stats = latest_exp['final_metrics']['dataset_analysis']
                        except:
                            pass  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
                
                # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—æ¨å®šï¼ˆGemmaã®å ´åˆï¼‰
                model_type = "gemma2"  
                
                # æ¨å¥¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
                ollama_recommendation = recommender.recommend_ollama_parameters(model_type, dataset_stats)
                
                st.info(f"ğŸ’¡ **æ¨å¥¨è¨­å®š**: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç‰¹æ€§ï¼ˆ{dataset_stats.get('total_samples', 'N/A')}ä»¶ã€ç‰¹å®šçŸ¥è­˜{'ã‚ã‚Š' if dataset_stats.get('has_specific_knowledge') else 'ãªã—'}ï¼‰ã«åŸºã¥ãæœ€é©åŒ–")
                
                col_rec1, col_rec2 = st.columns([2, 1])
                with col_rec1:
                    temp_rec = ollama_recommendation.get('temperature', 0.7)
                    top_p_rec = ollama_recommendation.get('top_p', 0.9)
                    st.write(f"Temperature: {temp_rec}, Top-P: {top_p_rec}")
                with col_rec2:
                    apply_ollama_recommendations = st.checkbox("æ¨å¥¨è¨­å®šã‚’é©ç”¨", value=True, key="ollama_rec")
                
            except Exception as e:
                st.warning(f"æ¨å¥¨æ©Ÿèƒ½ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
                apply_ollama_recommendations = False
                ollama_recommendation = {'temperature': 0.7, 'top_p': 0.9, 'num_ctx': 4096, 'repeat_penalty': 1.1}
        
        # ç”¨é€”åˆ¥ãƒ—ãƒªã‚»ãƒƒãƒˆ
        use_cases = ['general', 'creative', 'precise', 'translation', 'coding']
        use_case = st.selectbox(
            "ç”¨é€”",
            options=use_cases,
            format_func=lambda x: {
                'general': 'ä¸€èˆ¬ç”¨é€”',
                'creative': 'å‰µä½œ',
                'precise': 'æ­£ç¢ºæ€§é‡è¦–',
                'translation': 'ç¿»è¨³',
                'coding': 'ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°'
            }[x]
        )
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ææ¡ˆã‚’å–å¾—ï¼ˆå¾“æ¥ã®æ–¹æ³•ã‚‚ä½µç”¨ï¼‰
        optimization = integrator.optimize_parameters(model_name, use_case)
        recommended = optimization['recommended_parameters']
        
        # AIæ¨å¥¨ãŒæœ‰åŠ¹ãªå ´åˆã¯ãã¡ã‚‰ã‚’å„ªå…ˆ
        if apply_ollama_recommendations and 'ollama_recommendation' in locals():
            final_recommendations = ollama_recommendation
        else:
            final_recommendations = recommended
        
        col1, col2 = st.columns(2)
        
        with col1:
            temperature = st.slider(
                "Temperature",
                0.0, 2.0, final_recommendations.get('temperature', 0.7),
                help="å¿œç­”ã®ãƒ©ãƒ³ãƒ€ãƒ æ€§"
            )
            top_p = st.slider(
                "Top P",
                0.0, 1.0, final_recommendations.get('top_p', 0.9),
                help="æ ¸ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"
            )
        
        with col2:
            repeat_penalty = st.slider(
                "Repeat Penalty",
                1.0, 2.0, final_recommendations.get('repeat_penalty', 1.1),
                help="ç¹°ã‚Šè¿”ã—æŠ‘åˆ¶"
            )
            num_ctx = st.selectbox(
                "Context Length",
                options=[2048, 4096, 8192, 16384],
                index=1 if final_recommendations.get('num_ctx', 4096) == 4096 else 0,
                help="ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·"
            )
        
        # ãƒ¢ãƒ‡ãƒ«ç™»éŒ²å®Ÿè¡Œ
        if st.button("ğŸ¤– ãƒ¢ãƒ‡ãƒ«ç™»éŒ²", type="primary", disabled=not model_name):
            parameters = {
                'temperature': temperature,
                'top_p': top_p,
                'repeat_penalty': repeat_penalty,
                'num_ctx': num_ctx
            }
            
            with st.spinner("ãƒ¢ãƒ‡ãƒ«ç™»éŒ²ä¸­..."):
                result = integrator.create_model_from_gguf(
                    str(selected_gguf),
                    model_name,
                    system_prompt,
                    parameters
                )
                
                if result['success']:
                    st.success("âœ… ãƒ¢ãƒ‡ãƒ«ç™»éŒ²å®Œäº†ï¼")
                    
                    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
                    st.subheader("ğŸ§ª ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ")
                    test_result = integrator.test_model(model_name)
                    
                    if test_result['success']:
                        st.success("âœ… ãƒ†ã‚¹ãƒˆæˆåŠŸ")
                        st.write("**ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:**", test_result['prompt'])
                        st.write("**å¿œç­”:**", test_result['response'])
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            st.metric("ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°", test_result.get('eval_count', 0))
                        with col2:
                            duration_ms = test_result.get('eval_duration', 0) / 1_000_000
                            st.metric("å¿œç­”æ™‚é–“", f"{duration_ms:.0f} ms")
                    else:
                        st.error(f"âŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {test_result['error']}")
                    
                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢
                    if 'quantized_model_path' in st.session_state:
                        del st.session_state['quantized_model_path']
                        
                else:
                    st.error(f"âŒ ç™»éŒ²ã‚¨ãƒ©ãƒ¼: {result['error']}")
    
    # ãƒ¢ãƒ‡ãƒ«ç®¡ç†
    if models:
        st.subheader("ğŸ—‘ï¸ ãƒ¢ãƒ‡ãƒ«ç®¡ç†")
        
        model_to_delete = st.selectbox(
            "å‰Šé™¤ã™ã‚‹ãƒ¢ãƒ‡ãƒ«",
            options=[''] + [model['name'] for model in models],
            format_func=lambda x: "é¸æŠã—ã¦ãã ã•ã„" if x == '' else x
        )
        
        if model_to_delete:
            # å‰Šé™¤ç¢ºèªãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã‚’å…ˆã«è¡¨ç¤º
            confirm_delete = st.checkbox(f"ã€Œ{model_to_delete}ã€ã®å‰Šé™¤ã‚’ç¢ºèªã—ã¾ã™")
            
            if confirm_delete and st.button("ğŸ—‘ï¸ ãƒ¢ãƒ‡ãƒ«å‰Šé™¤", type="secondary"):
                with st.spinner("ãƒ¢ãƒ‡ãƒ«å‰Šé™¤ä¸­..."):
                    result = integrator.delete_model(model_to_delete)
                    
                    if result['success']:
                        st.success(f"âœ… {result['message']}")
                        st.rerun()
                    else:
                        st.error(f"âŒ {result['error']}")
    
    # ===============================
    # âœ… ãƒ¢ãƒ‡ãƒ«è»¢é€ç”¨tarãƒœãƒ¼ãƒ«ä½œæˆ
    # ===============================
    st.header("ğŸ“¦ ãƒ¢ãƒ‡ãƒ«è»¢é€")
    st.write("ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä»–ã®PCã«è»¢é€ã™ã‚‹ãŸã‚ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚’ä½œæˆã—ã¾ã™ã€‚")
    
    # åˆ©ç”¨å¯èƒ½ãªå®Ÿé¨“ã‚’å–å¾—
    experiment_tracker = ExperimentTracker()
    experiments = experiment_tracker.list_experiments()
    completed_experiments = [exp for exp in experiments if exp.get('status') == 'completed']
    
    if completed_experiments:
        col1, col2 = st.columns(2)
        
        with col1:
            selected_exp = st.selectbox(
                "è»¢é€ã™ã‚‹å®Ÿé¨“ã‚’é¸æŠ",
                options=completed_experiments,
                format_func=lambda x: f"{x['id'][:8]} - {x.get('model_name', 'unknown')}"
            )
        
        with col2:
            output_name = st.text_input(
                "ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å", 
                value=f"finetuned-model-{selected_exp['id'][:8]}"
            )
        
        if st.button("ğŸ“¦ è»¢é€ç”¨ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä½œæˆ", type="primary"):
            with st.spinner("ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä½œæˆä¸­..."):
                archive_result = create_transfer_archive(selected_exp['id'], output_name)
                
                if archive_result['success']:
                    st.success(f"âœ… ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä½œæˆå®Œäº†: {archive_result['archive_path']}")
                    st.info(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {archive_result['size_mb']:.1f} MB")
                    
                    # è»¢é€æ‰‹é †ã‚’è¡¨ç¤º
                    st.subheader("ğŸ“‹ è»¢é€æ‰‹é †")
                    transfer_commands = f"""# 1. è»¢é€å…ˆPCã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼
scp {archive_result['archive_path']} user@target-pc:/path/to/destination/

# 2. è»¢é€å…ˆPCã§ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚’å±•é–‹
tar -xzf {archive_result['filename']}

# 3. Ollamaãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
ollama create my-finetuned-model -f <(cat <<EOF
FROM ./models/quantized/{archive_result['gguf_filename']}

SYSTEM "ã‚ãªãŸã¯è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯Œãªæ—¥æœ¬èªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€æ­£ç¢ºã§æœ‰ç›Šãªå›ç­”ã‚’æ—¥æœ¬èªã§æä¾›ã—ã¦ãã ã•ã„ã€‚å›ç­”ã¯åˆ†ã‹ã‚Šã‚„ã™ãç°¡æ½”ã«ã¾ã¨ã‚ã€å¿…è¦ã«å¿œã˜ã¦å…·ä½“ä¾‹ã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚"

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER num_ctx 4096
PARAMETER repeat_penalty 1.1
EOF
)"""
                    st.code(transfer_commands, language="bash")
                else:
                    st.error(f"âŒ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä½œæˆã‚¨ãƒ©ãƒ¼: {archive_result['error']}")
    else:
        st.info("ğŸ“‹ è»¢é€å¯èƒ½ãªå®Œäº†æ¸ˆã¿å®Ÿé¨“ãŒã‚ã‚Šã¾ã›ã‚“")
    
    # ===============================
    # ğŸ§¹ ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    # ===============================
    st.header("ğŸ§¹ ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—")
    st.write("ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¦ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å®¹é‡ã‚’ç¢ºä¿ã—ã¾ã™ã€‚")
    
    # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ä½¿ç”¨é‡ã‚’å–å¾—
    cleanup_info = get_cleanup_info()
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœ", f"{cleanup_info['finetuned_size_gb']:.2f} GB")
    with col2:
        st.metric("é‡å­åŒ–ãƒ•ã‚¡ã‚¤ãƒ«", f"{cleanup_info['quantized_size_gb']:.2f} GB")
    with col3:
        st.metric("å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿", f"{cleanup_info['experiments_size_gb']:.2f} GB")
    
    st.warning("âš ï¸ HuggingFaceãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆgemma-2-2b-itç­‰ï¼‰ã¯å‰Šé™¤ã•ã‚Œã¾ã›ã‚“ã€‚")
    
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    cleanup_options = st.multiselect(
        "å‰Šé™¤å¯¾è±¡ã‚’é¸æŠ",
        [
            "ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœ (models/finetuned/)",
            "é‡å­åŒ–ãƒ•ã‚¡ã‚¤ãƒ« (models/quantized/)",
            "å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ (experiments/)",
            "MLXã‚­ãƒ£ãƒƒã‚·ãƒ¥ (models/cache/)",
            "GGUFã‚­ãƒ£ãƒƒã‚·ãƒ¥ (models/gguf_cache/)"
        ],
        default=["ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœ (models/finetuned/)", "é‡å­åŒ–ãƒ•ã‚¡ã‚¤ãƒ« (models/quantized/)"]
    )
    
    if cleanup_options:
        confirm_cleanup = st.checkbox("âš ï¸ å‰Šé™¤ã‚’ç¢ºèªã—ã¾ã™ï¼ˆã“ã®æ“ä½œã¯å–ã‚Šæ¶ˆã›ã¾ã›ã‚“ï¼‰")
        
        if confirm_cleanup and st.button("ğŸ—‘ï¸ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ", type="secondary"):
            with st.spinner("ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œä¸­..."):
                cleanup_result = perform_cleanup(cleanup_options)
                
                if cleanup_result['success']:
                    st.success(f"âœ… ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: {cleanup_result['freed_gb']:.2f} GB å‰Šé™¤")
                    st.rerun()
                else:
                    st.error(f"âŒ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {cleanup_result['error']}")


def experiments_page():
    """å®Ÿé¨“å±¥æ­´ãƒšãƒ¼ã‚¸"""
    st.title("ğŸ“ˆ å®Ÿé¨“å±¥æ­´")
    
    experiment_tracker = ExperimentTracker()
    
    # çµ±è¨ˆæƒ…å ±
    stats = experiment_tracker.get_summary_stats()
    
    st.subheader("ğŸ“Š çµ±è¨ˆæƒ…å ±")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ç·å®Ÿé¨“æ•°", stats['total_experiments'])
    with col2:
        st.metric("å®Œäº†", stats['completed'])
    with col3:
        st.metric("å¤±æ•—", stats['failed'])
    with col4:
        st.metric("å®Ÿè¡Œä¸­", stats['running'])
    
    if stats['total_experiments'] > 0:
        success_rate = stats['success_rate'] * 100
        avg_duration_hours = stats['average_duration_seconds'] / 3600
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("æˆåŠŸç‡", f"{success_rate:.1f}%")
        with col2:
            st.metric("å¹³å‡å®Ÿè¡Œæ™‚é–“", f"{avg_duration_hours:.1f} æ™‚é–“")
    
    # å®Ÿé¨“ãƒ•ã‚£ãƒ«ã‚¿
    st.subheader("ğŸ” å®Ÿé¨“æ¤œç´¢")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        status_filter = st.selectbox(
            "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹",
            options=['ã™ã¹ã¦', 'completed', 'failed', 'running'],
            format_func=lambda x: {
                'ã™ã¹ã¦': 'ã™ã¹ã¦',
                'completed': 'å®Œäº†',
                'failed': 'å¤±æ•—',
                'running': 'å®Ÿè¡Œä¸­'
            }[x]
        )
    
    with col2:
        # ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—
        all_experiments = experiment_tracker.list_experiments()
        model_names = list(set([exp.get('model_name', '') for exp in all_experiments]))
        
        model_filter = st.selectbox(
            "ãƒ¢ãƒ‡ãƒ«",
            options=['ã™ã¹ã¦'] + model_names,
            format_func=lambda x: x.split('/')[-1] if x != 'ã™ã¹ã¦' else x
        )
    
    with col3:
        limit = st.number_input("è¡¨ç¤ºä»¶æ•°", 5, 100, 20)
    
    # å®Ÿé¨“ãƒªã‚¹ãƒˆå–å¾—
    experiments = experiment_tracker.list_experiments(
        status=None if status_filter == 'ã™ã¹ã¦' else status_filter,
        model_name=None if model_filter == 'ã™ã¹ã¦' else model_filter,
        limit=limit
    )
    
    if experiments:
        st.subheader("ğŸ“‹ å®Ÿé¨“ä¸€è¦§")
        
        # å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
        exp_data = []
        for exp in experiments:
            duration_hours = (exp.get('duration_seconds', 0) / 3600) if exp.get('duration_seconds') else None
            
            exp_data.append({
                'ID': exp['id'][:8],
                'ãƒ¢ãƒ‡ãƒ«': exp['model_name'].split('/')[-1],
                'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹': {
                    'completed': 'âœ… å®Œäº†',
                    'failed': 'âŒ å¤±æ•—',
                    'running': 'ğŸ”„ å®Ÿè¡Œä¸­'
                }.get(exp['status'], exp['status']),
                'ä½œæˆæ—¥æ™‚': exp['created_at'][:19],
                'å®Ÿè¡Œæ™‚é–“': f"{duration_hours:.1f}h" if duration_hours else "N/A",
                'æœ€çµ‚æå¤±': exp.get('final_metrics', {}).get('final_loss', 'N/A')
            })
        
        df = pd.DataFrame(exp_data)
        
        # ã‚¯ãƒªãƒƒã‚¯å¯èƒ½ãªå®Ÿé¨“é¸æŠ
        selected_exp_id = st.selectbox(
            "è©³ç´°ã‚’è¡¨ç¤ºã™ã‚‹å®Ÿé¨“",
            options=[''] + [exp['id'] for exp in experiments],
            format_func=lambda x: "é¸æŠã—ã¦ãã ã•ã„" if x == '' else f"{x[:8]} - {next(e['model_name'].split('/')[-1] for e in experiments if e['id'] == x)}"
        )
        
        st.dataframe(df, use_container_width=True)
        
        # å®Ÿé¨“è©³ç´°è¡¨ç¤º
        if selected_exp_id:
            exp_detail = experiment_tracker.get_experiment(selected_exp_id)
            
            if exp_detail:
                st.subheader(f"ğŸ“„ å®Ÿé¨“è©³ç´°: {selected_exp_id[:8]}")
                
                # åŸºæœ¬æƒ…å ±
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write("**åŸºæœ¬æƒ…å ±**")
                    st.write(f"â€¢ ãƒ¢ãƒ‡ãƒ«: {exp_detail['model_name']}")
                    st.write(f"â€¢ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {Path(exp_detail['dataset_path']).name}")
                    st.write(f"â€¢ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {exp_detail['status']}")
                    st.write(f"â€¢ ä½œæˆæ—¥æ™‚: {exp_detail['created_at'][:19]}")
                
                with col2:
                    st.write("**å®Ÿè¡Œæƒ…å ±**")
                    if exp_detail.get('duration_seconds'):
                        st.write(f"â€¢ å®Ÿè¡Œæ™‚é–“: {exp_detail['duration_seconds']/3600:.1f} æ™‚é–“")
                    if exp_detail.get('output_dir'):
                        st.write(f"â€¢ å‡ºåŠ›: {exp_detail['output_dir']}")
                    if exp_detail.get('error'):
                        st.error(f"ã‚¨ãƒ©ãƒ¼: {exp_detail['error']}")
                
                # è¨­å®šè¡¨ç¤º
                with st.expander("âš™ï¸ è¨­å®š"):
                    st.json(exp_detail.get('config', {}))
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
                metrics = experiment_tracker.get_experiment_metrics(selected_exp_id)
                
                if metrics:
                    st.subheader("ğŸ“Š å­¦ç¿’æ›²ç·š")
                    
                    # æå¤±ã‚°ãƒ©ãƒ•
                    metrics_df = pd.DataFrame([
                        {
                            'step': m['step'],
                            'loss': m['metrics'].get('loss', 0),
                            'avg_loss': m['metrics'].get('avg_loss', 0)
                        }
                        for m in metrics if 'loss' in m['metrics']
                    ])
                    
                    if not metrics_df.empty:
                        fig = px.line(
                            metrics_df, 
                            x='step', 
                            y=['loss', 'avg_loss'],
                            title='Training Loss'
                        )
                        st.plotly_chart(fig, use_container_width=True)
                
                # ãƒ­ã‚°è¡¨ç¤º
                logs = experiment_tracker.get_experiment_logs(selected_exp_id)
                
                if logs:
                    with st.expander("ğŸ“œ ãƒ­ã‚°"):
                        for log in logs[-10:]:  # æœ€æ–°10ä»¶
                            timestamp = log['timestamp'][:19]
                            level = log['level']
                            message = log['message']
                            st.text(f"[{timestamp}] {level}: {message}")
                
                # å®Ÿé¨“æ“ä½œ
                col1, col2 = st.columns(2)
                
                with col1:
                    if st.button("ğŸ“ å®Ÿé¨“ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"):
                        export_path = f"./experiments/{selected_exp_id}_export.json"
                        if experiment_tracker.export_experiment(selected_exp_id, export_path):
                            st.success(f"âœ… ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {export_path}")
                        else:
                            st.error("âŒ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå¤±æ•—")
                
                with col2:
                    if st.button("ğŸ—‘ï¸ å®Ÿé¨“å‰Šé™¤", type="secondary"):
                        if st.checkbox("å‰Šé™¤ã‚’ç¢ºèªã—ã¾ã™", key=f"delete_{selected_exp_id}"):
                            if experiment_tracker.delete_experiment(selected_exp_id):
                                st.success("âœ… å®Ÿé¨“ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
                                st.rerun()
                            else:
                                st.error("âŒ å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ")
    else:
        st.info("å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")


def settings_page():
    """è¨­å®šãƒšãƒ¼ã‚¸"""
    st.title("âš™ï¸ è¨­å®š")
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ç·¨é›†
    st.subheader("ğŸ“ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«")
    
    tab1, tab2 = st.tabs(["åŸºæœ¬è¨­å®š", "ãƒ¢ãƒ‡ãƒ«è¨­å®š"])
    
    with tab1:
        try:
            with open('./config/default_config.yaml', 'r', encoding='utf-8') as f:
                config_content = f.read()
            
            edited_config = st.text_area(
                "default_config.yaml",
                value=config_content,
                height=400,
                help="YAMLå½¢å¼ã§è¨­å®šã‚’ç·¨é›†ã—ã¦ãã ã•ã„"
            )
            
            if st.button("ğŸ’¾ åŸºæœ¬è¨­å®šã‚’ä¿å­˜"):
                try:
                    # YAMLæ¤œè¨¼
                    yaml.safe_load(edited_config)
                    
                    with open('./config/default_config.yaml', 'w', encoding='utf-8') as f:
                        f.write(edited_config)
                    
                    st.success("âœ… è¨­å®šã‚’ä¿å­˜ã—ã¾ã—ãŸ")
                    
                except yaml.YAMLError as e:
                    st.error(f"âŒ YAMLå½¢å¼ã‚¨ãƒ©ãƒ¼: {e}")
                except Exception as e:
                    st.error(f"âŒ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
                    
        except Exception as e:
            st.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    with tab2:
        try:
            with open('./config/models.yaml', 'r', encoding='utf-8') as f:
                models_content = f.read()
            
            edited_models = st.text_area(
                "models.yaml",
                value=models_content,
                height=400,
                help="åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®è¨­å®š"
            )
            
            if st.button("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’ä¿å­˜"):
                try:
                    # YAMLæ¤œè¨¼
                    yaml.safe_load(edited_models)
                    
                    with open('./config/models.yaml', 'w', encoding='utf-8') as f:
                        f.write(edited_models)
                    
                    st.success("âœ… è¨­å®šã‚’ä¿å­˜ã—ã¾ã—ãŸ")
                    
                except yaml.YAMLError as e:
                    st.error(f"âŒ YAMLå½¢å¼ã‚¨ãƒ©ãƒ¼: {e}")
                except Exception as e:
                    st.error(f"âŒ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
                    
        except Exception as e:
            st.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
    st.subheader("ğŸ’» ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±")
    
    memory_monitor = MemoryMonitor()
    memory_info = memory_monitor.get_memory_info()
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.metric("ç·ãƒ¡ãƒ¢ãƒª", f"{memory_info['total_gb']:.1f} GB")
        st.metric("ä½¿ç”¨ãƒ¡ãƒ¢ãƒª", f"{memory_info['used_gb']:.1f} GB")
        st.metric("åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª", f"{memory_info['available_gb']:.1f} GB")
    
    with col2:
        st.metric("ä½¿ç”¨ç‡", f"{memory_info['percent']:.1f}%")
        st.metric("ç©ºããƒ¡ãƒ¢ãƒª", f"{memory_info['free_gb']:.1f} GB")
    
    # ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡
    st.subheader("ğŸ’¾ ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡")
    
    directories = {
        "ãƒ‡ãƒ¼ã‚¿": "./data",
        "ãƒ¢ãƒ‡ãƒ«": "./models",
        "å®Ÿé¨“": "./experiments",
        "ãƒ­ã‚°": "./logs"
    }
    
    for name, path in directories.items():
        if os.path.exists(path):
            size = sum(
                os.path.getsize(os.path.join(dirpath, filename))
                for dirpath, dirnames, filenames in os.walk(path)
                for filename in filenames
            ) / (1024**3)  # GB
            
            st.metric(name, f"{size:.2f} GB")
        else:
            st.metric(name, "0 GB")


# ãƒ¡ã‚¤ãƒ³é–¢æ•°
def main():
    try:
        # ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒ¡ãƒ‹ãƒ¥ãƒ¼
        selected_page = sidebar_menu()
        
        # ãƒšãƒ¼ã‚¸è¡¨ç¤º
        if selected_page == "home":
            home_page()
        elif selected_page == "dataset":
            dataset_page()
        elif selected_page == "training":
            training_page()
        elif selected_page == "quantization":
            quantization_page()
        elif selected_page == "ollama":
            ollama_page()
        elif selected_page == "experiments":
            experiments_page()
        elif selected_page == "settings":
            settings_page()
            
    except Exception as e:
        st.error(f"ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
        logger.exception("Application error")


if __name__ == "__main__":
    main()