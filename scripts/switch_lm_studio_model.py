#!/usr/bin/env python3
"""
LM Studioã®ãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆã‚’è‡ªå‹•åŒ–ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import subprocess
import time
import requests
import json
from pathlib import Path

FINETUNED_MODEL_PATH = "/Users/matsbaccano/Projects/clone/mlx-finetuning/works/mlx_finetuning_1754901424/fused_model"
LM_STUDIO_API_BASE = "http://localhost:1234/v1"

def check_lm_studio_running():
    """LM Studio APIã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    try:
        response = requests.get(f"{LM_STUDIO_API_BASE}/models", timeout=5)
        return response.status_code == 200
    except:
        return False

def get_current_models():
    """ç¾åœ¨ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—"""
    try:
        response = requests.get(f"{LM_STUDIO_API_BASE}/models")
        if response.status_code == 200:
            return response.json()
    except:
        pass
    return None

def test_model_knowledge():
    """ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’æ¸ˆã¿çŸ¥è­˜ã‚’ãƒ†ã‚¹ãƒˆ"""
    test_questions = [
        "ã‚ãªãŸã®æ‰€å±ã—ã¦ã„ã‚‹ä¼šç¤¾ã¯ï¼Ÿ",
        "ä¼šç¤¾ã®è¨­ç«‹å¹´ã¯ï¼Ÿ"
    ]
    
    results = {}
    for question in test_questions:
        try:
            response = requests.post(f"{LM_STUDIO_API_BASE}/chat/completions", 
                json={
                    "model": "fused_model",
                    "messages": [{"role": "user", "content": question}],
                    "temperature": 0.1,
                    "max_tokens": 100
                }, timeout=30)
            
            if response.status_code == 200:
                answer = response.json()["choices"][0]["message"]["content"]
                results[question] = answer
            else:
                results[question] = f"ã‚¨ãƒ©ãƒ¼: {response.status_code}"
        except Exception as e:
            results[question] = f"ã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    return results

def main():
    print("ğŸ” LM Studio ãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 50)
    
    # LM Studio APIç¢ºèª
    if not check_lm_studio_running():
        print("âŒ LM Studio APIã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã¾ã›ã‚“")
        print("LM Studioã‚’é–‹ã„ã¦ã€ãƒ­ãƒ¼ã‚«ãƒ«ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã—ã¦ãã ã•ã„")
        return
    
    print("âœ… LM Studio APIã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ä¸­")
    
    # ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ç¢ºèª
    current_models = get_current_models()
    if current_models:
        print(f"ğŸ“‹ ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«: {[m['id'] for m in current_models['data']]}")
    
    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ç¢ºèª
    model_path = Path(FINETUNED_MODEL_PATH)
    if not model_path.exists():
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {FINETUNED_MODEL_PATH}")
        return
    
    print(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ç¢ºèª: {model_path}")
    
    # ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜ãƒ†ã‚¹ãƒˆ
    print("\nğŸ§ª ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜ãƒ†ã‚¹ãƒˆ:")
    results = test_model_knowledge()
    for question, answer in results.items():
        print(f"Q: {question}")
        print(f"A: {answer}")
        print()
    
    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿çŸ¥è­˜ã®åˆ¤å®š
    has_company_knowledge = any("ãƒ†ãƒƒã‚¯ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³" in str(answer) for answer in results.values())
    has_year_knowledge = any("2020" in str(answer) for answer in results.values())
    
    if has_company_knowledge and has_year_knowledge:
        print("âœ… ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã¯æ­£ã—ã„ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿çŸ¥è­˜ã‚’æŒã£ã¦ã„ã¾ã™")
        return
    else:
        print("âŒ ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã¯ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿çŸ¥è­˜ã‚’æŒã£ã¦ã„ã¾ã›ã‚“")
    
    print("\nğŸ“ æ‰‹å‹•ã§ãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆã¦ãã ã•ã„:")
    print("1. LM Studioã‚’é–‹ã")
    print("2. ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰")
    print(f"3. ä»¥ä¸‹ã®ãƒ‘ã‚¹ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰:")
    print(f"   {FINETUNED_MODEL_PATH}")
    print("4. ãƒ¢ãƒ‡ãƒ«åã‚’ 'fused_model' ã«è¨­å®š")
    print("5. ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å†å®Ÿè¡Œã—ã¦ç¢ºèª")

if __name__ == "__main__":
    main()